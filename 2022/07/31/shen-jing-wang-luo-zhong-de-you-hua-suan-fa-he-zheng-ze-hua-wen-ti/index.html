

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="灰一">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文主要参考邱锡鹏老师的nndl-book，作为笔记使用 神经网络与深度学习（第7-10讲）（更新至：第9讲 无监督学习）_哔哩哔哩_bilibili  非凸优化问题 高维空间中常常遇到的都是非凸函数 非凸优化问题中难点有：  如何逃离鞍点 会出现平坦最小值  鞍点 在某些维度上是最高点,另一些维度上是最低点.其一阶导数为0,二阶梯度Hessian矩阵不是半正定矩阵.   鞍点示例  平坦最小值">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络中的优化算法和正则化问题">
<meta property="og:url" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/index.html">
<meta property="og:site_name" content="灰一茶话会">
<meta property="og:description" content="本文主要参考邱锡鹏老师的nndl-book，作为笔记使用 神经网络与深度学习（第7-10讲）（更新至：第9讲 无监督学习）_哔哩哔哩_bilibili  非凸优化问题 高维空间中常常遇到的都是非凸函数 非凸优化问题中难点有：  如何逃离鞍点 会出现平坦最小值  鞍点 在某些维度上是最高点,另一些维度上是最低点.其一阶导数为0,二阶梯度Hessian矩阵不是半正定矩阵.   鞍点示例  平坦最小值">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%201.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%202.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%203.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%204.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%205.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%206.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%207.png">
<meta property="og:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%208.png">
<meta property="article:published_time" content="2022-07-31T05:50:09.000Z">
<meta property="article:modified_time" content="2022-07-31T06:04:51.140Z">
<meta property="article:author" content="灰一">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="DL">
<meta property="article:tag" content="数学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled.png">
  
  
  
  <title>神经网络中的优化算法和正则化问题 - 灰一茶话会</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"ash-one.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络中的优化算法和正则化问题"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-31 13:50" pubdate>
          2022年7月31日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          55 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">神经网络中的优化算法和正则化问题</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>本文主要参考邱锡鹏老师的nndl-book，作为笔记使用</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV19u411d7r3">神经网络与深度学习（第7-10讲）（更新至：第9讲
无监督学习）_哔哩哔哩_bilibili</a></p>
</blockquote>
<h1 id="非凸优化问题">非凸优化问题</h1>
<p>高维空间中常常遇到的都是非凸函数</p>
<p>非凸优化问题中难点有：</p>
<ul>
<li>如何逃离鞍点</li>
<li>会出现平坦最小值</li>
</ul>
<h2 id="鞍点">鞍点</h2>
<p>在某些维度上是最高点,另一些维度上是最低点.其一阶导数为0,二阶梯度Hessian矩阵不是半正定矩阵.</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled.png" srcset="/img/loading.gif" lazyload alt="鞍点示例">
<figcaption aria-hidden="true">鞍点示例</figcaption>
</figure>
<h2 id="平坦最小值">平坦最小值</h2>
<blockquote>
<p>由于DL中的参数非常多且具有冗余,单个参数对最终的损失影响比较小,这会导致损失函数在局部最小解附近通常是一个平坦的区域</p>
</blockquote>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%201.png" srcset="/img/loading.gif" lazyload alt="平坦最小值和尖锐最小值">
<figcaption aria-hidden="true">平坦最小值和尖锐最小值</figcaption>
</figure>
<ul>
<li>一个平坦最小值的邻域内,所有点对应的训练损失都比较接近</li>
<li>大部分局部最小解是等价的</li>
<li>局部最小解对应的训练损失有可能非常接近于全局最小解训练损失</li>
</ul>
<aside>
💡
在训练神经网络时,可以用局部最小解代替全局最优解,以防止过拟合,鲁棒性更好
</aside>
<h1 id="神经网络优化算法">神经网络优化算法</h1>
<h2 id="随机梯度下降法sgd">随机梯度下降法(SGD)</h2>
<p>小批量随机梯度下降,每次选择k个样本计算偏导数的平均数,然后在梯度反向上更新参数</p>
<h2 id="批量大小调整">批量大小调整</h2>
<p>不影响梯度的期望,但会影响方差.</p>
<ul>
<li><p>batch越大,方差越小,学习率需要设置较大</p></li>
<li><p>batch越小,需要设置较小的学习率,否则模型不会收敛</p></li>
<li><p>NOTE:batch和学习率可遵守线性缩放原则</p>
<p>batch变为m倍,学习率变为<span class="math inline">\(1/m\)</span></p></li>
</ul>
<aside>
💡
batch越大通常训练的效率越高,batch越小由于随机性更高,可能会获得更好的泛化能力
</aside>
<h2 id="梯度估计修正">梯度估计修正</h2>
<ul>
<li><p>Momentum Method 动量法</p>
<p>用之前累积的动量来代替真正的梯度（负梯度的加权移动平均）</p>
<aside>
<p>💡 每次的迭代方向相当于加速度，类似二阶梯度</p>
</aside>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta_t =\rho \Delta \theta_{t-1}-\alpha g_t=-\alpha \sum
\limits _{\tau=1}^{t} \rho^{t-\tau}g_{\tau}
  \]</span></p></li>
<li><p>Nesterov 加速梯度</p></li>
<li><p>梯度截断</p>
<p>当某个时刻梯度（的模）过大如NaN，就把梯度的模限定在某个区间，将超过区间的部分进行截断</p>
<ul>
<li><p>按值截断</p>
<p>例如取<span class="math inline">\([a,b]\)</span>区间的值</p>
<p><span class="math display">\[
  g_t=\max(\min(g_t,b),a)
  \]</span></p></li>
<li><p>按模截断</p>
<p>把模的最大值限定为b</p>
<p><span class="math display">\[
  g_t=\frac{b}{||g_t||}g_t
  \]</span></p></li>
</ul></li>
<li><p>Adam算法</p>
<p>使用动量法优化梯度，加上RMSprop自适应调整学习率</p>
<p>先计算两个移动平均：</p>
<p><span class="math display">\[
  M_t = \beta _1M_{t-1}+(1-\beta_1)g_t
  \]</span></p>
<p><span class="math display">\[
  G_t = \beta_2G_{t-1}+(1-\beta_2)g_t \odot g_t
  \]</span></p>
<p>当t=1时，<span class="math inline">\(M_1\)</span>应该等于<span class="math inline">\(g_1\)</span>，但是在上述公式中不正确，于是加入偏差修正如下：</p>
<p><span class="math display">\[
  \hat M_t = \frac {M_t}{1-\beta_1^t}
  \]</span></p>
<p><span class="math display">\[
  \hat G_t=\frac{G_t}{1-\beta_2^t}
  \]</span></p>
<p>更新参数内容：</p>
<p><span class="math display">\[
  \Delta \theta_t = - \frac{\alpha}{\sqrt {G_t+\epsilon}}\hat M_t
  \]</span></p></li>
</ul>
<h2 id="学习率调整">学习率调整</h2>
<ul>
<li><p>学习率衰减</p>
<ul>
<li>阶梯衰减</li>
<li>逆时衰减:<span class="math inline">\(\alpha_t = \alpha_0
\frac{1}{1+\beta \times t}\)</span></li>
<li>指数衰减</li>
<li>余弦衰减</li>
</ul></li>
<li><p>周期性学习率调整</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%202.png" srcset="/img/loading.gif" lazyload alt="周期性学习率调整">
<figcaption aria-hidden="true">周期性学习率调整</figcaption>
</figure>
<p>跳出局部最优或鞍点的情况,找到更平坦的局部最优</p></li>
<li><p>学习率预热</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%203.png" srcset="/img/loading.gif" lazyload alt="学习率预热示例">
<figcaption aria-hidden="true">学习率预热示例</figcaption>
</figure></li>
<li><p>AdaGrad算法</p>
<p>借鉴L2正则化的思想，在第t次迭代的时候，先计算每个参数梯度平方的累计值。</p>
<p><span class="math display">\[
  G_t=\sum \limits _{\tau}^{t}g_{\tau}\odot g_{\tau}
  \]</span></p>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta _{t} = - \frac {\alpha}{\sqrt {G_t+\epsilon}}\odot g_t
  \]</span></p>
<p>其中<span class="math inline">\(\alpha\)</span>是初始学习率，<span class="math inline">\(\epsilon\)</span>是为保持数值稳定一个小常数（大小通常是<span class="math inline">\(e^{-7}\sim
e^{-10}\)</span>），公式内的运算都是按元素进行的操作。</p>
<blockquote>
<p>from nndl-book:</p>
<p>在 AdaGrad
算法中，如果某个参数的偏导数累积比较大，其学习率相对较小;相反，如果其偏导数累积较小，其学习率相对较大.但整体是随着迭代次数的增加，学习率逐渐缩小.
AdaGrad
算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点.</p>
</blockquote></li>
<li><p>RMSprop算法</p>
<p>避免AdaGrad算法中学习率不断单调下降导致过早衰减的问题。</p>
<p>参数更新的差值和AdaGrad算法是相同的：</p>
<p><span class="math display">\[
  \Delta \theta _{t} = - \frac {\alpha}{\sqrt {G_t+\epsilon}}\odot g_t
  \]</span></p>
<p>区别在于计算<span class="math inline">\(G_t\)</span>:</p>
<p><span class="math display">\[
  \begin{aligned}
  G_{t} &amp;=\beta G_{t-1}+(1-\beta) \boldsymbol{g}_{t} \odot
\boldsymbol{g}_{t} \\
  &amp;=(1-\beta) \sum_{\tau=1}^{t} \beta^{t-\tau} \boldsymbol{g}_{\tau}
\odot \boldsymbol{g}_{\tau}
  \end{aligned}
  \]</span></p>
<p>由于参数的学习率变化由单调衰减变成了指数加权的移动平均，既可以变大也可以变小。</p></li>
<li><p>AdaDelta算法</p>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta_{t}=-\frac{\sqrt{\Delta
X_{t-1}^{2}+\epsilon}}{\sqrt{G_{t}+\epsilon}} \mathbf{g}_{t}
  \]</span></p>
<p>其中<span class="math inline">\(G_t\)</span>和RMSprop算法相同：</p>
<p><span class="math display">\[
  \begin{aligned}
  G_{t} &amp;=\beta G_{t-1}+(1-\beta) \boldsymbol{g}_{t} \odot
\boldsymbol{g}_{t} \\
  &amp;=(1-\beta) \sum_{\tau=1}^{t} \beta^{t-\tau} \boldsymbol{g}_{\tau}
\odot \boldsymbol{g}_{\tau}
  \end{aligned}
  \]</span></p>
<p>分子上：</p>
<p><span class="math display">\[
  \Delta X_{t-1}^{2}=\beta_{1} \Delta
X_{t-2}^{2}+\left(1-\beta_{1}\right) \Delta \theta_{t-1} \odot \Delta
\theta_{t-1}
  \]</span></p>
<p>其中<span class="math inline">\(\beta _1\)</span>是衰减率</p>
<p>由于将原本的学习率修改为了带衰减率的移动平均，能够抑制住学习率的波动。</p></li>
</ul>
<h2 id="参数初始化">参数初始化</h2>
<p>当使用梯度下降法优化网络参数时，参数的初始值非常重要，直接影响到网络的优化效率和泛化能力</p>
<p>神经网络的参数不能初始化为0，会出现对称权重问题，导致层内所有权重相同。</p>
<p>因此对于参数的初始化也有许多方法。</p>
<ul>
<li>预训练初始化</li>
<li>随机初始化
<ul>
<li><p>基于固定方差的参数初始化</p>
<ul>
<li><p>高斯分布初始化</p>
<p>使用方差和均值的高斯分布</p></li>
<li><p>均匀分布初始化</p>
<p>采用在区间<span class="math inline">\([-r,r]\)</span>内采用均匀分布进行初始化</p></li>
</ul></li>
<li><p>基于方差缩放的参数初始化</p>
<p><span class="math inline">\(M_l\)</span>是第<span class="math inline">\(l\)</span>层的神经元个数</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%204.png" srcset="/img/loading.gif" lazyload alt="基于方差缩放的参数初始化设置">
<figcaption aria-hidden="true">基于方差缩放的参数初始化设置</figcaption>
</figure></li>
<li><p>正交初始化</p>
<p>由于上述两种初始化对每个参数都是独立采样，依旧存在梯度消失或梯度爆炸问题。</p>
<p>此处我们从最开始就构建一个L层的等宽线性网络：</p>
<p><span class="math display">\[
  \bold y= \bold W^L \bold W^{L-1} \dots \bold W^1 \bold x
  \]</span></p>
<p>在反向传播中误差项<span class="math inline">\(\delta\)</span>的反向传播公式为：<span class="math inline">\(\delta ^{l-1}=(W^l)^T\delta ^l\)</span></p>
<p><strong>范数保持性：</strong>指误差项在反向传播中的范数保持不变，满足<span class="math inline">\(||\delta^{l-1}||^2=||\delta^l||^2=||(W^l)^T\delta^l||^2\)</span>。</p>
<p>于是在初始化就可以使用均值为0，方差为<span class="math inline">\(1/M\)</span>的高斯分布来做初始化</p>
<hr>
<p>或者采用更直接的方式：直接将 <span class="math inline">\(W^l\)</span>初始化为正交矩阵，即<span class="math inline">\(W^l(W^l)^T=I\)</span></p>
<ol type="1">
<li>y地方用均值为0，方差为1的高斯分布初始化一个矩阵</li>
<li>将这个矩阵用奇异值分解得到两个正交矩阵，用其中之一作为权重矩阵</li>
</ol>
<aside>
<p>💡 正交初始化通常用在RNN的循环边上的权重矩阵</p>
</aside></li>
</ul></li>
</ul>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="尺度不变性scale-invariance">尺度不变性（Scale Invariance）</h3>
<p>对于机器算法中的数据，在缩放部分特征或者全部特征后，不影响学习和预测。</p>
<p>例如：将厘米单位的特征，缩放为毫米单位。</p>
<p><em>NOTE:KNN算法的尺度不变性较差，因为其计算的欧式距离对数据要求高</em></p>
<p>不同输入特征的尺度差异较大会带来两个问题：</p>
<ol type="1">
<li>参数初始化困难</li>
<li>优化效率较低</li>
</ol>
<h3 id="归一化规范化-normalization">归一化（规范化 Normalization）</h3>
<ul>
<li><p><strong>最小最大值规范化</strong>：</p>
<p>找到数据中的最小最大值，将所有数据缩放到这个范围内</p></li>
<li><p><strong>标准化（Z值规范化）</strong>：</p>
<p>对每一维数据计算均值和方差</p>
<p><span class="math display">\[
  \mu = \frac{1}{N} \sum \limits _{n=1}^Nx^n
  \]</span></p>
<p><span class="math display">\[
  \sigma ^2 = \frac{1}{N}\sum \limits _{n=1}^N (x^n-\mu)^2
  \]</span></p>
<p>然后将特征<span class="math inline">\(x^n\)</span>进行变换：</p>
<p><span class="math display">\[
  \hat x^n = \frac{x^n-\mu}{\sigma}
  \]</span></p>
<p>标准差<span class="math inline">\(\sigma\)</span>做分母不为零的含义：</p>
<p>表示这一维度的特征相同，不重要可以直接去掉</p></li>
<li><p><strong>白化（whitening，PCA）</strong>:</p>
<p>去除所有成分之间的相关性</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%205.png" srcset="/img/loading.gif" lazyload alt="标准化和白化">
<figcaption aria-hidden="true">标准化和白化</figcaption>
</figure></li>
</ul>
<h3 id="逐层规范化">逐层规范化</h3>
<p>ML中的常用方法，应用到DL的目的是：</p>
<ol type="1">
<li><p>更好的尺度不变性</p>
<p>解决内部协变量偏移问题，防止在深度学习中某一个层的输入分布的微小变化引起更深层中的较大偏移。</p></li>
<li><p>更平滑的优化地形</p></li>
</ol>
<ul>
<li>规范化方法
<ul>
<li><p><strong>批量规范化（Batch Normalization）</strong></p>
<p>对于一个神经网络，第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(z^l\)</span>，神经元的输出为<span class="math inline">\(a^l\)</span>，即：</p>
<p><span class="math display">\[
  a^l = f(z^l)=f(Wa^{l-1}+b)
  \]</span></p>
<p>对于一个样本数为k的批量样本，计算其方差和均值：</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;\boldsymbol{\mu}_{\mathcal{B}}=\frac{1}{K} \sum_{k=1}^{K}
\boldsymbol{z}^{(k, l)} \\
  &amp;\boldsymbol\sigma_{\mathcal{B}}^{2}=\frac{1}{K}
\sum_{k=1}^{K}\left(\boldsymbol{z}^{(k,
l)}-\boldsymbol{\mu}_{\mathcal{B}}\right) \odot\left(\boldsymbol{z}^{(k,
l)}-\boldsymbol{\mu}_{\mathcal{B}}\right) .
  \end{aligned}
  \]</span></p>
<p>批量规范化：</p>
<p><span class="math display">\[
  \begin{aligned}
  \hat{\boldsymbol{z}}^{l}
&amp;=\frac{\boldsymbol{z}^{l}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}
\odot \boldsymbol{\gamma}+\boldsymbol{\beta} \\
  &amp; \triangleq \mathrm{BN}_{\gamma,
\boldsymbol{\beta}}\left(\boldsymbol{z}^{l}\right),
  \end{aligned}
  \]</span></p>
<p>其中<span class="math inline">\(\boldsymbol{\gamma}\)</span>是缩放参数向量，<span class="math inline">\(\boldsymbol{\beta}\)</span>是平移参数向量，都是可学习参数。能够增强规范化的能力。</p>
<aside>
<p>💡 批量规范化可以作为神经网络的一层（BN层）直接加入网络结构中。</p>
</aside></li>
<li><p><strong>层规范化（Layer Normalization）</strong></p>
<p>不看batch的内容，对一层内所有神经元进行规范化</p>
<p>计算层内均值和方差：</p>
<p><span class="math display">\[
  \begin{aligned}
  \mu^{l} &amp;=\frac{1}{M_{l}} \sum_{i=1}^{M_{l}} z_{i}^{l} \\
  (\sigma^{l})^{2} &amp;=\frac{1}{M_{l}}
\sum_{i=1}^{M_{l}}\left(z_{i}^{l}-\mu^{l}\right)^{2}
  \end{aligned}
  \]</span></p>
<p>层规范化：</p>
<p><span class="math display">\[
  \begin{aligned}
  \hat{\boldsymbol{z}}^{l}
&amp;=\frac{\boldsymbol{z}^{l}-\mu^{l}}{\sqrt{(\sigma^{(l)})^{2}+\epsilon}}
\odot \boldsymbol{\gamma}+\boldsymbol\beta \\
  &amp; \triangleq \mathrm{LN}_{\boldsymbol {\gamma,
\beta}}\left(\boldsymbol{z}^{l}\right)
  \end{aligned}
  \]</span></p>
<aside>
<p>💡
层规范化也可以作为神经网络的一层（LN层）直接加入网络结构中。应用较多。</p>
</aside>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%206.png" srcset="/img/loading.gif" lazyload alt="BN和LN的区别">
<figcaption aria-hidden="true">BN和LN的区别</figcaption>
</figure>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%207.png" srcset="/img/loading.gif" lazyload alt="各种规范化的对比">
<figcaption aria-hidden="true">各种规范化的对比</figcaption>
</figure></li>
<li><p>权重规范化</p></li>
<li><p>局部响应规范化</p></li>
</ul></li>
</ul>
<h2 id="超参数优化">超参数优化</h2>
<h3 id="超参数">超参数</h3>
<ul>
<li>层数</li>
<li>每层神经元个数</li>
<li>激活函数</li>
<li>学习率（以及动态调整算法）</li>
<li>正则化系数</li>
<li>mini-batch大小</li>
</ul>
<h3 id="优化方法">优化方法</h3>
<ul>
<li><p>网格搜索</p>
<p>对每一个超参数取几个”经验值“，比如对于学习率<span class="math inline">\(\alpha\)</span>，可以设置</p>
<p><span class="math display">\[
  \alpha  \in \{0.01,0.1,0.5,1.0\}
  \]</span></p></li>
<li><p>随机搜索</p>
<p>对超参数进行随机组合</p></li>
<li><p>贝叶斯优化</p></li>
<li><p>动态资源分配</p></li>
<li><p>神经架构搜索</p></li>
</ul>
<h2 id="网络正则化">网络正则化</h2>
<p>由于神经网络的拟合能力强，会将模型过度参数化，会导致模型泛化性差。为提高神经网络的泛化能力通常要加入正则化。</p>
<p><strong>正则化</strong>就是所有损害优化方法的方法。通常分为两种方式：</p>
<ol type="1">
<li>增加优化约束：L1、L2约束</li>
<li>干扰优化过程：提前停止、随机梯度下降、暂退法、权重衰减。</li>
</ol>
<h3 id="早停法">早停法</h3>
<p>引入<strong>验证集（Validation
Dataset）</strong>测试每一次迭代的参数时候在验证集上最优，如果在验证集上错误率不再下降就停止迭代。防止过拟合。</p>
<h3 id="权重衰减">权重衰减</h3>
<p>限制权重取值范围。在每次参数更新时，引入一个衰减系数<span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[
\theta_t=(1-\beta)\theta_{t-1}-\alpha \bold g_t
\]</span></p>
<h3 id="暂退法dropout">暂退法（Dropout）</h3>
<p>有一个神经层<span class="math inline">\(y=f(Wx+b)\)</span>，引入一个随机的掩蔽函数<span class="math inline">\(mask(x)=m\odot x\)</span>，<span class="math inline">\(m \in \{0,1\}^D\)</span>.得到神经层的输出<span class="math inline">\(y=f(Wmask(x)+b)\)</span>.</p>
<aside>
💡
可以理解为从原始网络中采样得到一个子网络，如果有n个神经元，则可以采样出<span class="math inline">\(2^n\)</span>个子网络。
</aside>
<h3 id="ell_1和ell_2正则化"><span class="math inline">\(\ell_1\)</span>和<span class="math inline">\(\ell_2\)</span>正则化</h3>
<p>在原本的优化问题中加入<span class="math inline">\(\ell_1\)</span><span class="math inline">\(\ell_2\)</span>范数防止过拟合</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^N
\mathcal{L}\left(y^{(n)}, f\left(\boldsymbol{x}^{(n)} ;
\theta\right)\right) \\
&amp;\text { s.t. } \quad \ell_{p}(\theta) \leq 1
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\ell_2\)</span>正则化的参数更新结果为：<span class="math inline">\(\theta_t =
\theta_{t-1}-\alpha(g_t+\lambda\theta_{t-1})=(1-\alpha\lambda)\theta_{t-1}-\alpha
g_t\)</span></p>
<p>形式上和权重衰减的类似：<span class="math inline">\(\theta_t=(1-\beta)\theta_{t-1}-\alpha \bold
g_t\)</span></p>
<aside>
💡 在SGD中 <span class="math inline">\(\ell_2\)</span>
正则化和权重衰减是等价的，但在复杂的优化算法如Adam中则不能等价。
</aside>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%208.png" srcset="/img/loading.gif" lazyload alt="不同范数的约束条件示例">
<figcaption aria-hidden="true">不同范数的约束条件示例</figcaption>
</figure>
<h1 id="总结">总结</h1>
<table>

<thead>
<tr class="header">
<th>模型</th>
<th>优化</th>
<th>正则化</th>
<th>隐式正则化</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>用ReLU作为激活函数</td>
<td>SGD+mini-batch(动态学习率、Adam算法优先)</td>
<td>早停法</td>
<td>SGD</td>
</tr>
<tr class="even">
<td>残差链接</td>
<td>每次迭代都重新随机排序</td>
<td>暂退法</td>
<td>批量规范化</td>
</tr>
<tr class="odd">
<td>逐层规范化</td>
<td>数据预处理（规范化）</td>
<td>权重衰减</td>
<td><span class="math inline">\(\ell_1\)</span>和<span class="math inline">\(\ell_2\)</span>正则化</td>
</tr>
<tr class="even">
<td></td>
<td>参数初始化（预训练）</td>
<td></td>
<td>数据增强</td>
</tr>
</tbody>
</table>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%AD%A6%E4%B9%A0/" class="category-chain-item">学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/ML/">#ML</a>
      
        <a href="/tags/DL/">#DL</a>
      
        <a href="/tags/%E6%95%B0%E5%AD%A6/">#数学</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>神经网络中的优化算法和正则化问题</div>
      <div>https://ash-one.github.io/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>灰一</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月31日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/31/apple-silicon-macbook-pro-xin-ji-pei-zhi/" title="Apple Silicon MacBook Pro新机配置">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Apple Silicon MacBook Pro新机配置</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/30/ji-qi-xue-xi-shu-xue-ji-chu-shu-zhi-ji-suan-he-zui-you-hua-li-lun/" title="机器学习数学基础③数值计算和最优化理论">
                        <span class="hidden-mobile">机器学习数学基础③数值计算和最优化理论</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>

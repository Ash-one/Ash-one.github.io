<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo博客在matery主题下插入mermaid流程图</title>
    <url>/2022/07/12/hexo-bo-ke-zai-matery-zhu-ti-xia-cha-ru-mermaid-liu-cheng-tu/</url>
    <content><![CDATA[<h1 id="hexo博客在matery主题下插入mermaid流程图">Hexo博客在matery主题下插入mermaid流程图</h1>
<blockquote>
<p>本文解决hexo本身无法渲染mermaid流程图的问题,本质是加载对应的js文件</p>
<p>其他主题可以直接参考https://github.com/webappdevelp/hexo-filter-mermaid-diagrams</p>
</blockquote>
<p>官方指定在<code>after-footer.ejs</code>文件中插入以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.mermaid.enable) &#123; %&gt;</span><br><span class="line">  &lt;script src=&#x27;https://unpkg.com/mermaid@&lt;%= theme.mermaid.version %&gt;/dist/mermaid.min.js&#x27;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">    if (window.mermaid) &#123;</span><br><span class="line">      mermaid.initialize(&#123;theme: &#x27;forest&#x27;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure>
<p>而实际上在matery主题中没有该文件</p>
<h2 id="解决方法">解决方法</h2>
<h3 id="第一步-增加js代码">第一步 增加js代码</h3>
<p>在matery主题路径<code>themes/hexo-theme-matery/layout/_partial</code>下找到<code>footer.ejs</code></p>
<p>直接在该文件的最后添加如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.mermaid.enable) &#123; %&gt;</span><br><span class="line">  &lt;script src=&#x27;https://unpkg.com/mermaid/dist/mermaid.min.js&#x27;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">    if (window.mermaid) &#123;</span><br><span class="line">      mermaid.initialize(&#123;theme: &#x27;forest&#x27;&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure>
<p><strong>注意到去掉了原本中间的版本控制,
因为版本选择可能会导致加载的js不能正常显示typora中的mermaid图</strong></p>
<ul>
<li>如果想要修改主题则直接在这里修改theme后的内容为如下: <em>default |
dark | forest | neutral</em></li>
</ul>
<h3 id="第二步-增加yml文本">第二步 增加yml文本</h3>
<p>在<em>主题的配置文件</em>下<code>themes/hexo-theme-matery/_config.yml</code>增加文本</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mermaid:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="结语">结语</h2>
<p>不出意外到这里进行测试就可以见到hexo能够加载出来mermaid图了(撒花)</p>
<p>如果没能成功加载, 可以查看源代码查找mermaid,
看看增加的ejs代码是否成功渲染</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
        <tag>typora</tag>
      </tags>
  </entry>
  <entry>
    <title>Flutter学习笔记</title>
    <url>/2021/09/28/flutter-xue-xi-bi-ji/</url>
    <content><![CDATA[<h1 id="flutter学习笔记">Flutter学习笔记</h1>
<h2 id="statelesswidget和statefulwidget">StatelessWidget和StatefulWidget</h2>
<ul>
<li>StatelessWidget在创建之后将不会更改，想要更改只能new一个新的做替换。<br>
</li>
<li>StatefulWidget通过在State类中调用setState((){})更新视图，触发State.build，将整个组件重新绘制，<strong>同时会导致所有子组件重新构造生成，该结点的兄弟结点组件也会被重新构造</strong>。</li>
</ul>
<h2 id="开发时如何选择">开发时如何选择</h2>
<ul>
<li>优先使用 StatelessWidget<br>
</li>
<li>含有大量子 Widget（如根布局、次根布局）慎用 SatefulWidget</li>
<li>尽量在叶子节点时使用 StatefulWidget</li>
<li>将会调用到setState((){})
的代码尽可能的和要更新的视图封装在一个尽可能小的模块里</li>
<li>如果一个Widget需要reBuild，那么它的子节点、兄弟节点、兄弟节点的子节点应该尽可能少</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Flutter</tag>
        <tag>Dart</tag>
      </tags>
  </entry>
  <entry>
    <title>Python基础知识点</title>
    <url>/2021/09/18/python-ji-chu-zhi-shi-dian/</url>
    <content><![CDATA[<h1 id="python基础知识点">Python基础知识点</h1>
<blockquote>
<p>本篇为给高二学生上课用的复习内容, 高二学生都能看懂!!!</p>
</blockquote>
<h2 id="数据类型6个">数据类型6个</h2>
<ol type="1">
<li><strong>整型 int</strong><br>
数学中的整数，如1，-8080，0<br>
</li>
<li><strong>实型（浮点数）float</strong><br>
数学中的实数，如3.14，9e-4<br>
</li>
<li><strong>字符串 string</strong><br>
以引号开始结束的一串字符，如<code>'this is a string'</code><br>
</li>
<li><strong>布尔型 bool</strong><br>
只有<code>True</code>和<code>False</code>两个值<br>
</li>
<li><strong>列表 list</strong><br>
以中括号开始结束的一种数据类型，如<code>[1,2,3,4,5]</code><br>
</li>
<li><strong>字典 dictionary</strong><br>
以大括号开始结束，存储内容为键值对，如<code>&#123;'name':'wow','age':17&#125;</code><br>
取出下列字典中的'name'键对应的值： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d=&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;wow&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">17</span>&#125;  </span><br><span class="line">d[<span class="string">&#x27;name&#x27;</span>]=</span><br></pre></td></tr></table></figure> #### 索引与切片
请写出下列索引与切片的结果 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">info=[<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;what&#x27;</span>,<span class="number">5432</span>]  </span><br><span class="line">info2=[ [<span class="string">&#x27;world&#x27;</span>,<span class="string">&#x27;you&#x27;</span>],[<span class="number">231</span>,<span class="string">&#x27;how&#x27;</span>] ]   </span><br><span class="line">info[<span class="number">1</span>]=  </span><br><span class="line">info[<span class="number">1</span>][<span class="number">0</span>]=  </span><br><span class="line">info[<span class="number">1</span>][<span class="number">1</span>:<span class="number">3</span>]=  </span><br><span class="line">info2[<span class="number">0</span>]=  </span><br><span class="line">info2[<span class="number">1</span>][<span class="number">0</span>]=  </span><br><span class="line">info2[<span class="number">1</span>][<span class="number">1</span>][<span class="number">0</span>]=  </span><br><span class="line">info2[<span class="number">1</span>][<span class="number">1</span>][<span class="number">1</span>:<span class="number">3</span>]=  </span><br></pre></td></tr></table></figure> ***</li>
</ol>
<h2 id="运算符">运算符</h2>
<h3 id="算数运算符">算数运算符</h3>
<p><code>+-*/</code>加减乘除 <code>//</code>整除，结果取整数部分<br>
<code>%</code>取余数 <code>**</code>乘方</p>
<p><em>算数运算符得到的结果为数</em> ### 关系运算符
<code>&gt;&lt;</code>大于小于 <code>&gt;= &lt;=</code>大于等于
小于等于<br>
<code>==</code>等于<code>!=</code>不等于<br>
<code>in</code>包含于</p>
<p><em>关系运算得到的结果为布尔型，即<code>True</code>和<code>False</code></em></p>
<h3 id="逻辑运算符">逻辑运算符</h3>
<p><code>and</code>与运算，只有两个<code>True</code>结果才为<code>True</code><br>
<code>or</code>
或运算，只要有一个<code>True</code>结果就为<code>True</code><br>
<code>not</code>非运算，<code>not True</code>结果为<code>False</code></p>
<p><em>逻辑运算得到的结果为布尔型，即<code>True</code>和<code>False</code></em></p>
<hr>
<h2 id="变量和赋值">变量和赋值</h2>
<p><strong>常量</strong>：0，3.14，60000，pi，e等数学上的客观数字<br>
<strong>变量</strong>：在编程中声明/定义的一个代号，用于存储数据。变量名可以包括字母、数字、下划线，不可以用数字开头</p>
<p><strong>赋值语句</strong><br>
<code>=</code>用于赋值，将等号右侧的值赋给左侧的变量<br>
&gt;n=0 #将0赋值给变量n<br>
n=n+1 #将n+1的值赋给变量n<br>
n+=1 #将n+1的值赋给变量n，即n的值增加1</p>
<p><em>注意：<code>==</code>和<code>=</code>在用法上完全不同</em></p>
<h2 id="常用函数">常用函数</h2>
<p><code>print(x)</code>输出x的值<br>
<code>input(x)</code>将x的值输出在屏幕上，并读取用户的输入，以字符串存储<br>
<code>int(x)</code>将x转换为整型<br>
<code>float(x)</code>将x转换为浮点数</p>
<hr>
<h2 id="分支结构">分支结构</h2>
<blockquote>
<p>if 条件:<br>
  语句<br>
elif 条件:<br>
  语句<br>
else:<br>
  语句</p>
</blockquote>
<p>例：<br>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> x%<span class="number">2</span>==<span class="number">0</span>:  </span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;偶数&#x27;</span>)  </span><br><span class="line">esle:  </span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;奇数&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="循环结构">循环结构</h2>
<h4 id="for循环">for循环</h4>
<p>序列遍历完成后退出循环 &gt;for 变量 in 序列:<br>
  循环体</p>
<p>例：<br>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="number">0</span>  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">101</span>,<span class="number">1</span>):  </span><br><span class="line">		s+=i  </span><br><span class="line">		<span class="built_in">print</span>(i)  </span><br></pre></td></tr></table></figure> #### while循环
循环条件为<code>True</code>时进入循环，否则退出循环<br>
&gt;while 循环条件:<br>
  循环体</p>
<p>例：<br>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="number">0</span>  </span><br><span class="line">i=<span class="number">0</span>  </span><br><span class="line"><span class="keyword">while</span> i&lt;=<span class="number">100</span>:  </span><br><span class="line">    s+=i</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="built_in">print</span>(s)</span><br></pre></td></tr></table></figure></p>
<p><strong><em>代码除了横向阅读，也要纵向阅读</em></strong><br>
<strong><em>横向阅读看<code>功能</code>，纵向阅读看<code>结构</code></em></strong><br>
<strong><em>注意循环体和分支结构语句块的缩进</em></strong></p>
<hr>
<h2 id="习题">习题</h2>
<p>使用for循环和while循环两种方式计算[100,1000]中所有奇数的和<br>
请在下方书写<br>
<em>提示：判断一个数是偶数还是奇数是用余数的方式</em></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>信息技术</tag>
        <tag>支教</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo快速使用指北</title>
    <url>/2020/10/12/hexo-kuai-su-shi-yong-zhi-bei/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<p>可以指定模板</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br><span class="line">$ hexo new post <span class="string">&quot;这是一个新的笔记&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<h3 id="命令可以组合使用">命令可以组合使用</h3>
<blockquote>
<p>hexo g &amp;&amp; hexo d &amp;&amp; hexo s</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>机器学习数学基础①线性代数</title>
    <url>/2022/07/24/ji-qi-xue-xi-shu-xue-ji-chu-xian-xing-dai-shu/</url>
    <content><![CDATA[<h1 id="机器学习数学基础①线性代数">机器学习数学基础①线性代数</h1>
<p>线性代数的核心问题是将一个向量空间的子空间映射到另一个向量空间的子空间,
这个过程使用过的方法叫<strong>线性变换</strong>,
而<strong>矩阵</strong>就是两个向量空间之间线性变换的表达形式</p>
<h2 id="基础概念">基础概念</h2>
<ul>
<li><p>矩阵</p></li>
<li><p>向量</p></li>
<li><p>矩阵乘法: 点积</p></li>
<li><p>矩阵转置</p></li>
<li><p>逆矩阵: 两个方阵相乘结果为单位阵, 记为<span class="math inline">\(A^{-1}\)</span>,称<span class="math inline">\(A\)</span>为可逆矩阵</p></li>
<li><p>正交: 向量<span class="math inline">\(x\)</span>与<span class="math inline">\(y\)</span>正交, 则<span class="math inline">\(x
\cdot y=0\)</span>, 意味着垂直</p></li>
<li><p>正交矩阵: 对于方阵<span class="math inline">\(A \in \mathbb{R}^{n
\times n}\)</span>, 若有<span class="math inline">\(AA^T=I_n=A^TA\)</span>, 其中<span class="math inline">\(A^{-1}=A^T\)</span>, 意味着其转置等于其逆的矩阵,
即<strong>正交矩阵</strong></p></li>
<li><p>对角矩阵</p></li>
<li><p>正定矩阵: 有<span class="math inline">\(n\times
n\)</span>实对称矩阵<span class="math inline">\(A\)</span>和n维非零向量<span class="math inline">\(x\)</span>, 如果<span class="math inline">\(x^TAx&gt;0\)</span>则称<span class="math inline">\(A\)</span>为<strong>正定矩阵</strong>, 如果<span class="math inline">\(x^TAx\geqslant 0\)</span>则称<span class="math inline">\(A\)</span>为<strong>半正定矩阵</strong>.</p>
<blockquote>
<p>正定矩阵<span class="math inline">\(A\)</span>保证变换后的向量<span class="math inline">\(Ax\)</span>与原向量<span class="math inline">\(x\)</span>都位于超平面的同一侧.</p>
</blockquote></li>
</ul>
<h2 id="重要概念">重要概念</h2>
<h3 id="范数">范数</h3>
<p>向量的范数就是向量的长度或大小, 通项公式为<span class="math inline">\(||\vec
x||=(\Sigma_{i=1}^{n}|x^i|^p)^{1/p}\)</span></p>
<p><em>在ML中常用来限制模型复杂度, 防止过拟合等</em></p>
<p>其中p为范数的阶, ML中常用两个:</p>
<ul>
<li>p=1,称为一阶范数<span class="math inline">\(l_1\)</span>范数
L1正则化, <span class="math inline">\(||\vec
x||=\Sigma_{i=1}^{n}|x^i|\)</span>, 表示向量<span class="math inline">\(x\)</span>中的各元素绝对值的和</li>
<li>p=2, 称为二阶范数<span class="math inline">\(l_2\)</span>范数
L2正则化,<span class="math inline">\(||\vec x||=\sqrt{
\Sigma_{i=1}^{n}|x^i|^2}\)</span>, 表示向量中的元素平方和再开平方</li>
</ul>
<h3 id="柯西不等式">柯西不等式</h3>
<p>由余弦定理<span class="math inline">\(\vec a \cdot \vec b=|\vec
a||\vec b|\cos \theta\)</span>得: <span class="math display">\[
|\vec a \cdot \vec b|\leqslant |\vec a||\vec b|
\]</span></p>
<h2 id="矩阵运算常用技巧">矩阵运算常用技巧</h2>
<ol type="1">
<li><p>若<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是n阶方阵, 且<span class="math inline">\(A+B\)</span>可逆, 有: <span class="math display">\[
A(A+B)^{-1}B=B(A+B)^{-1}A
\]</span></p></li>
<li><p><strong>矩阵指数</strong> <span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是n阶方阵, 有:</p>
<ul>
<li><p><span class="math display">\[
  e^{A^{T}}=(e^A)^T
  \]</span></p></li>
<li><p>若<span class="math inline">\(AB=BA\)</span>, 则</p></li>
</ul>
<p><span class="math display">\[
e^Ae^B = e^Be^A = e^{A+B}
\]</span></p></li>
</ol>
<h2 id="张量和张量积">张量和张量积</h2>
<p><strong>张量</strong>是在ML中常用的概念,
可理解为存在[标量,向量,矩阵...]等形式的一种数据结构 张量积(tensor
product)又称克罗内克积(Kronecker product)</p>
<h3 id="定义">定义</h3>
<p><span class="math inline">\(A=(a_{ij})_{m\times n}\)</span> , <span class="math inline">\(B\)</span> 为<span class="math inline">\(p\times
q\)</span>矩阵, 张量积<span class="math inline">\(A\otimes
B\)</span>是一个<span class="math inline">\(mp\times nq\)</span>矩阵: $$
AB =</p>
<p>$$</p>
<h2 id="特征分解">特征分解</h2>
<p>是指将矩阵分为一组特征向量和特征值</p>
<h3 id="特征向量和特征值">特征向量和特征值</h3>
<p>一个可对角化的矩阵<span class="math inline">\(A\)</span>的<strong>特征向量</strong><span class="math inline">\(v\)</span> 有: <span class="math display">\[
Av= \lambda v
\]</span></p>
<blockquote>
<p>一个方阵与特征向量相乘 相当于 对特征向量进行缩放.</p>
</blockquote>
<p>标量<span class="math inline">\(\lambda\)</span>就是这个特征向量的<strong>特征值</strong></p>
<h3 id="奇异值分解svd">奇异值分解(SVD)</h3>
<p>由于特征分解要求矩阵<span class="math inline">\(A\)</span>是一个可对角化的矩阵, 要求很高</p>
<p>为将特征分解进行推广, 使用的方法叫"矩阵的奇异值分解", 对于一个<span class="math inline">\(m \times n\)</span>的矩阵<span class="math inline">\(A\)</span>: <span class="math display">\[
A= UDV^T
\]</span> <span class="math inline">\(U\)</span>是<span class="math inline">\(m\times m\)</span>方阵, <span class="math inline">\(D\)</span>是<span class="math inline">\(m\times
n\)</span>矩阵, <span class="math inline">\(V\)</span>是<span class="math inline">\(n\times n\)</span>方阵</p>
<p><span class="math inline">\(UV\)</span>都是正交矩阵, <span class="math inline">\(D\)</span>是对角矩阵, <span class="math inline">\(D\)</span>的对角线上的元素就是矩阵<span class="math inline">\(A\)</span>的<strong>奇异值</strong>,<span class="math inline">\(U\)</span>的列向量被称为<strong>左奇异向量</strong>,
<span class="math inline">\(V\)</span>的列向量被称为<strong>右奇异向量</strong></p>
<h2 id="距离计算">距离计算</h2>
<p>计算两个向量之间的距离, 可以反映之间的相似程度</p>
<p>现有两个n维变量 <span class="math display">\[
A= [x_{11},x_{12},\dots,x_{1n}]\\
B= [x_{21},x_{22},\dots,x_{2n}]
\]</span></p>
<h3 id="曼哈顿距离">1. 曼哈顿距离</h3>
<p>表示向量对应元素的距离和 <span class="math display">\[
d_{12} = \sum \limits _{k=1}^n |x_{1k}-x_{2k}|
\]</span></p>
<h3 id="欧氏距离">2. 欧氏距离</h3>
<p>就是L2范数, 表示对应元素的距离的平方和的开方 <span class="math display">\[
d_{12} = \sqrt{\sum \limits _{k=1}^n (x_{1k}-x_{2k})^2}
\]</span></p>
<h3 id="切比雪夫距离">3. 切比雪夫距离</h3>
<p>也是无穷范数, 表示各元素上距离中的最大值 <span class="math display">\[
d_{12} = \max (|x_{1k}-x_{2k}|)
\]</span></p>
<h3 id="余弦距离">4. 余弦距离</h3>
<p>两个方向的夹角余弦取值范围为[-1,1]</p>
<p>夹角余弦越大,表示两个向量夹角越小; 方向重合的两个向量,
余弦值为1;方向相反时, 余弦值为-1 <span class="math display">\[
\begin{aligned}
\cos \theta&amp;= \frac {AB}{|A||B|} \\
&amp;= \frac {\sum \limits _{k=1}^{n}x_{1k}x_{2k}}{\sqrt {\sum \limits
_{k=1}^{n}x_{1k}^2}\sqrt {\sum \limits _{k=1}^{n}x_{2k}^2}}
\end{aligned}
\]</span></p>
<h3 id="汉明距离">5. 汉明距离</h3>
<p>定义两个字符串中的不同位数的数目</p>
<p><span class="math inline">\(e.g.\)</span>
字符串<code>1111</code>和<code>1001</code>的汉明距离为2</p>
<h3 id="杰卡德相似系数">6. 杰卡德相似系数</h3>
<p>两个集合AB的交集元素在并集中的比例 <span class="math display">\[
J(A,B) = \frac {|A\cap B|}{|A\cup B|}
\]</span></p>
<h3 id="杰卡德距离">7. 杰卡德距离</h3>
<p>与杰卡德相似系数表示的内容相反 <span class="math display">\[
J_{\sigma}=1 - J(A,B) = 1- \frac {|A\cap B|}{|A\cup B|}
\]</span></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
        <tag>ML</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习数学基础②概率论和信息论</title>
    <url>/2022/07/26/ji-qi-xue-xi-shu-xue-ji-chu-gai-lu-lun-he-xin-xi-lun/</url>
    <content><![CDATA[<h1 id="机器学习数学基础②概率论与信息论">机器学习数学基础②概率论与信息论</h1>
<h1 id="基础概念">基础概念</h1>
<ul>
<li><p>随机变量:可以是连续的也可以是离散的</p></li>
<li><p>概率分布:符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性</p></li>
<li><p>联合概率分布:<span class="math inline">\(P(\mathrm x =x,\mathrm
y=y)\)</span>表示x=<span class="math inline">\(x\)</span>和y=<span class="math inline">\(y\)</span>同时发生的概率,简写为<span class="math inline">\(P(x,y)\)</span></p></li>
<li><p>概率函数和似然函数</p>
<ul>
<li><p>Note:区别</p>
<p>对于一个函数<span class="math inline">\(P(x|\theta)\)</span>,其中x表示一个具体的数据,<span class="math inline">\(\theta\)</span>表示模型的参数.</p>
<p>如果<span class="math inline">\(\theta\)</span>已知,x为变量,这个函数就是<strong>概率函数</strong>.表示对于不同的样本点x,其出现的概率是多少.</p>
<p>如果x已知,<span class="math inline">\(\theta\)</span>为变量,这个函数就是<strong>似然函数</strong>.表示对于不同的参数,出现x样本点的概率是多少.</p></li>
</ul></li>
<li><p>期望:<span class="math inline">\(E(X)=\sum \limits
_{k=1}^{n}x_kP(x_k), E(x)=\int xf(x)dx\)</span></p></li>
<li><p>方差:各个样本数据分别与平均数之差的平方和的平均数<span class="math inline">\(Var(x)\)</span></p></li>
<li><p>协方差:衡量随机变量X和Y之间的总体误差<span class="math inline">\(Cov(X,Y)\)</span></p></li>
</ul>
<h1 id="重要概念">重要概念</h1>
<ul>
<li><strong>条件概率</strong>:对于给定<span class="math inline">\(X=x\)</span>时发生<span class="math inline">\(Y=y\)</span>的概率记为<span class="math inline">\(P(Y=y|X=x)\)</span>,计算公式为<span class="math inline">\(P(Y=y|X=x)=\frac {P(x,y)}{P(X=x)}\)</span></li>
<li><strong>先验概率和后验概率</strong></li>
<li><strong>贝叶斯公式</strong>:利用先验概率计算后验概率
<ul>
<li><p>Note:贝叶斯公式推导</p>
<p><span class="math display">\[
  P(B|A)=\frac {P(AB)}{P(A)}, P(A|B)=\frac {P(AB)}{P(B)}
  \]</span></p>
<p><span class="math display">\[
  P(AB)=P(B|A)P(A)=P(A|B)P(B)
  \]</span></p>
<p><span class="math display">\[
  P(B|A)=\frac {P(B|A)P(B)}{P(A)}
  \]</span></p>
<p>全概率公式求<span class="math inline">\(P(A)\)</span></p>
<p><span class="math display">\[
  P(A)=\sum \limits _{i=1}^{N}P(A|B_i)P(B_i)
  \]</span></p>
<p>代入得到贝叶斯公式:</p>
<p><span class="math display">\[
  P(B_i|A)=\frac {P(A|B_i)P(B_i)}{\sum \limits _{i=1}^{N}P(A|B_i)P(B_i)}
  \]</span></p></li>
</ul></li>
<li><strong>最大似然估计(MLE):</strong>在”模型已定,参数<span class="math inline">\(\theta\)</span>未知”的情况下,通过观测数据来估计未知参数<span class="math inline">\(\theta\)</span>的一种方法,要求所有采样都是独立同分布.
<ul>
<li>Note:最大似然估计的使用
<ol type="1">
<li>写出似然函数</li>
<li>对似然函数取对数</li>
<li>两边同时求导(多个变量就求偏导)</li>
<li>令导数为0,解出似然方程</li>
</ol>
<ul>
<li><p>例子</p>
<figure>
<img src="https://s2.loli.net/2022/07/26/yXuBHYVrIP5LpNZ.png" alt="例题">
<figcaption aria-hidden="true">例题</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2022/07/26/LFjf2RnIMZbyuOo.png" alt="例题解答">
<figcaption aria-hidden="true">例题解答</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>
<h1 id="常见分布函数">常见分布函数</h1>
<ol type="1">
<li><p>0-1分布</p>
<p><span class="math display">\[
P(X=1)=p \\ P(X=0)=1-p
\]</span></p></li>
<li><p>几何分布</p>
<p>离散型概率分布.在n次伯努利试验中,前k-1次失败,第k次成功的概率,其概率分布函数为:<span class="math inline">\(P(X=k)=(1-p)^{k-1}p\)</span></p>
<p><span class="math display">\[
E(X)=\frac {1}{p} \\ Var(X)=\frac {1-p}{p^2}
\]</span></p></li>
<li><p>二项分布</p>
<p>n次伯努利试验中,每次试验只有两种可能结果,并且两种结果相互独立.在n次重复独立试验中事件发生k次的概率为:<span class="math inline">\(P(X=k)=C_n^kp^k(1-p)^{n-k}\)</span> <span class="math display">\[
E(X)=np \\ Var(X)=np(1-p)
\]</span></p></li>
<li><p>高斯分布/正态分布</p>
<p>ML中最常用的概率分布.其中 <span class="math inline">\(\mu\)</span>决定分布中心的位置,<span class="math inline">\(\sigma\)</span> 标准差决定正态分布的幅度.</p>
<figure>
<img src="https://s2.loli.net/2022/07/26/EXJlHW7oDNhqCbI.png" alt="正态分布例图">
<figcaption aria-hidden="true">正态分布例图</figcaption>
</figure>
<p><span class="math display">\[
X \sim N(\mu,\sigma^2)
\]</span></p></li>
<li><p>指数分布</p>
<p>指数分布是一种连续概率分布,可以用来表示独立随机事件发生的时间间隔.其中<span class="math inline">\(\lambda
&gt;0\)</span>表示每单位时间内发生事件的次数.</p>
<figure>
<img src="https://s2.loli.net/2022/07/26/afSYlZeE3C8jOKy.png" alt="指数分布例图">
<figcaption aria-hidden="true">指数分布例图</figcaption>
</figure>
<p><span class="math display">\[
X\sim Exp(\lambda)
\]</span></p>
<p>概率密度函数为:</p>
<p><span class="math display">\[
f(x;\lambda)=
\left\{
\begin{aligned}
&amp;\lambda e^{-\lambda x} &amp;x\geq0 \\
&amp;0&amp;x&lt;0
\end{aligned}
\right.
\]</span></p>
<p>特点为无记忆性,在任意时间段内某个事件发生的概率相同,结合期望理解.期望为:<span class="math inline">\(E(x)=\frac {1}{\lambda}\)</span>.</p></li>
<li><p>泊松分布</p>
<p>泊松分布是一种常见的离散概率分布.适合描述单位时间内随机事件发生的次数的概率分布.参数<span class="math inline">\(\lambda\)</span>是随机事件发生次数的数学期望.</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/650px-Poisson_pmf.svg.png" alt="泊松分布例图">
<figcaption aria-hidden="true">泊松分布例图</figcaption>
</figure>
<p>概率密度函数为:</p>
<p><span class="math display">\[
P(X=k)=\frac {e^{-\lambda}\lambda^k}{k!}
\]</span></p>
<p>泊松分布表示为:</p>
<p><span class="math display">\[
X \sim Pois(\lambda)
\]</span></p>
<p>性质有</p>
<ol type="1">
<li><p>期望与方差相等,都等于参数<span class="math inline">\(\lambda\)</span></p></li>
<li><p>两个独立且服从泊松分布的随机变量,其和仍然服从泊松分布</p>
<p><span class="math display">\[
X \sim Poisson(\lambda_1),Y \sim Possion(\lambda_2)\\
X+Y \sim Poisson(\lambda_1+\lambda_2)
\]</span></p></li>
</ol></li>
</ol>
<h1 id="信息论">信息论</h1>
<p><strong>自信息:</strong></p>
<p>一个事件x=<span class="math inline">\(x\)</span>的自信息为<span class="math inline">\(I(x)=-\log P(x)\)</span></p>
<p><strong>香农熵:</strong></p>
<p>一个分布P(x)的事件产生的信息总量的期望</p>
<p><span class="math display">\[
H(x)=\mathbb{E}_{X\sim P}[I(x)]=-\mathbb{E}_{X\sim P}[\log P(x)]
\]</span></p>
<p><strong>KL散度(KL divergence):</strong></p>
<p>衡量两个单独的概率分布P(x)和Q(x)的差异.KL散度非负,为0时表示分布相同.</p>
<p><span class="math display">\[
D_{KL}(P||Q)=\mathbb{E}_{X\sim P}\left[ \log \frac {P(x)}{Q(x)}
\right]=\mathbb{E}_{X\sim P}[\log P(x)- \log Q(x)]
\]</span></p>
<aside>
💡 KL散度中的P分布和Q分布不能交换顺序
</aside>
<p><strong>交叉熵(cross-entropy):</strong></p>
<p><span class="math display">\[
H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb{E}_{X\sim P}\log Q(x)
\]</span></p>
<aside>
💡 在信息论中0log0结果取0
</aside>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习数学基础③数值计算和最优化理论</title>
    <url>/2022/07/30/ji-qi-xue-xi-shu-xue-ji-chu-shu-zhi-ji-suan-he-zui-you-hua-li-lun/</url>
    <content><![CDATA[<h1 id="数值计算">数值计算</h1>
<h2 id="上溢和下溢">上溢和下溢</h2>
<p>在数字计算机中我们需要通过有限的位数表示无限多的实数,这意味着会引入误差.</p>
<ul>
<li><strong>下溢(underflow):</strong>当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除<strong>。</strong></li>
<li><strong>上溢(overflow):</strong>当大量级的数被近似为<span class="math inline">\(\infin\)</span>或<span class="math inline">\(-\infin\)</span>时发生上溢。进一步的运算通常将这些无限值变为非数字。</li>
</ul>
<p>避免数值上溢或者下溢的方法常用softmax函数.</p>
<figure>
<img src="/2022/07/30/ji-qi-xue-xi-shu-xue-ji-chu-shu-zhi-ji-suan-he-zui-you-hua-li-lun/softmax函数.png" alt="softmax函数">
<figcaption aria-hidden="true">softmax函数</figcaption>
</figure>
<p>softmax函数能够将上下接近溢出的数据<em>压缩</em>至0-1的范围内.</p>
<h1 id="最优化">最优化</h1>
<h2 id="最优化理论">最优化理论</h2>
<p><strong>经验风险最小化</strong></p>
<p>用最小的代价取得最大的收益.也就是我们今天常说的算法.</p>
<h2 id="凸集与凸集分离定理">凸集与凸集分离定理</h2>
<h3 id="凸集">1.凸集</h3>
<p>在实数域的向量空间中.如果一个集合中任意两点的连线上的点都在该集合中,则该集合为<strong>凸集</strong>.</p>
<figure>
<img src="https://pic1.zhimg.com/v2-608f89f47688c41e4c3f83cfad095c84_r.jpg" alt="凸集和非凸集">
<figcaption aria-hidden="true">凸集和非凸集</figcaption>
</figure>
<p>数学定义: 一个集合<span class="math inline">\(S\subset
R^T\)</span>,若对于任意两点<span class="math inline">\(x,y \in
S\)</span>,以及实数<span class="math inline">\(\lambda(0\leqslant
\lambda \leqslant 1)\)</span>,都有<span class="math inline">\(\lambda
x+(1-\lambda )y \in S\)</span>,则称集合S为凸集.</p>
<h3 id="超平面和半空间">2.超平面和半空间</h3>
<p>三维空间的<strong>超平面</strong>就是一个面(曲面),二维空间的超平面就是一条线(曲线),推广到n维空间.</p>
<p>超平面的某一个方向的所有样本组成<strong>半空间.</strong></p>
<h3 id="凸集分离定理">3.凸集分离定理</h3>
<p>所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：</p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-4116a3bda12faa5e2421ce27efb7fb71_1440w.jpg" alt="凸集分离">
<figcaption aria-hidden="true">凸集分离</figcaption>
</figure>
<h3 id="凸函数">4.凸函数</h3>
<p>凸函数是一个定义域在某个向量空间的凸子集上的实值函数.</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-f1b39d0aad4388433158679221f813d2_1440w.jpg" alt="凸函数">
<figcaption aria-hidden="true">凸函数</figcaption>
</figure>
<aside>
💡 在图中直线始终在凸函数曲线上方
</aside>
<p>数学定义:对于函数f(x),如果其定义域C是凸集,且对于<span class="math inline">\(\forall x,y \in C, 0\leqslant \alpha \leqslant
1\)</span>,有:</p>
<p><span class="math display">\[
f(\theta x+(1-\theta )y)\leqslant \theta f(x)+(1-\theta )f(y)
\]</span></p>
<p><strong>如果一个函数是凸函数,则其局部最优点就是其全局最优点.</strong></p>
<p>因为在ML中我们就是在求模型的全局最优点,所以一旦证明ML模型中的损失函数为凸函数,相当于我们只需要求它的局部最优点.</p>
<aside>
💡 求解非凸函数的全局最优点也就成了神经网络中的常见难题.
</aside>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>清河村支教回忆</title>
    <url>/2021/09/23/qing-he-cun-zhi-jiao-hui-yi/</url>
    <content><![CDATA[<p><em>成文于2021年8月11日离开清河村。</em><br>
  大包小包的行李，颠颠簸簸的山路，出发前忐忐忑忑的心情在进入清河村后终于有所平复，深山、公路、虫鸣、鸟叫，不仔细观察很难发现这座隐藏在山峦间的完全小学。设施落后、环境糟糕、蚊虫困扰、饮食不同，完全符合一个城市人对农村的印象。农村生活过的我自认为对环境要求不高，到了这里也难免有些烦闷。<br>
<img src="https://i.loli.net/2021/09/24/1mYu6dJgSeHTXDx.jpg" alt="清河村完全小学的清晨"> <img src="https://i.loli.net/2021/09/24/aDiCOMYG7mFnoRQ.jpg" alt="山">
  闲时无事，揣起相机，沿着新修的道路在村里漫步，才发现这里的一切都不是想象中的那样。道路上摩托车和大货车不断，即使急转或陡坡也都轻松通过，山路两旁都是村民种植的茶树，时不时有个斗笠冒出来，采摘着最新鲜的三片茶叶。村中邻里相互十分熟悉，抱着小孩，背着菜筐，在村公所门口摆着龙门阵，爽朗的笑声老远外就能听到。最让我意外的就是脱贫援建建设的网络设施已经成为了村民们最佳的休闲方式，在路边时不时就能听到短视频平台的神曲，想必不少村民也是某些平台的大V。</p>
<p><img src="https://i.loli.net/2021/09/24/idPy38m9OIn6hea.jpg" alt="清河村的晚霞"> <img src="https://i.loli.net/2021/09/24/qpxP5ImXbkTgFwz.jpg" alt="清河村的晚霞"></p>
<p>  这里离天很近，或者说这里就在所谓的“天上”，“天上”小学的学生自然都是“天兵天将”。这次梦想教室教授的对象是四年级的小学生，这天早上8点多我们还在硬板床上做着梦，“天兵天将”已经冲进了大门，在水泥操场上撒欢了。即使很多“天兵天将”穿着相同的“制服”和“盔甲”，明亮的眼神却在黝黑的皮肤中更显耀眼。或许是对未知事物的一丝丝恐惧，每当我用镜头想要捕捉到一张张笑脸时，他们会用远超我按下快门的速度表演一个变脸，迅速板起表情，我也只能悻悻地放下相机。
<img src="https://i.loli.net/2021/09/24/d5Iw9AcqlbP6NyT.jpg" alt="小学">
  对相机有所顾忌的学生们对待知识的态度就完全不同了，即使课上常常提到陌生的概念他们也十分愿意聆听学习，常常是一提问就好多双小手在空中挥舞。上过几天课后逐渐有学生不怕我的镜头了，下课期间几个好事的小男生上来摆弄我脚架上的相机快门，吸引了其他同学的注意，“天兵天将”便纷纷在相机里留下自己的鬼脸。最后一天，小孩子们拿着纸笔一个一个问我们的电话号码，看着一张张期待的小脸，一瞬间我就扭过了头，属实是不能被看见眼红啊。<br>
<img src="https://i.loli.net/2021/09/24/KBYmuUxLbeMypRw.jpg" alt="山路">
  总有人说，不要去支教，即使去教这些大山中的孩子，给他们看了外界的美好，他们也走不出去，或是因为能力，或是因为家庭，支教的故事只是给他们徒增烦恼罢了。对待短期支教，我更想做的是给他们送去新鲜的思想，告诉他们“绿水青山就是金山银山”，告诉他们梦想绝不渺小，梦想永远伟大。<br>
<img src="https://i.loli.net/2021/09/24/hnegQSNylC7tz25.jpg" alt="银河"></p>
<p>  我从不认为自己是一个运气很好的人，但是我为能来到这座深山中的小小村庄倍感荣幸。<br>
<img src="https://i.loli.net/2021/09/24/AT6KXrjhfLvO7Rb.jpg" alt="村里的兔兔"> <img src="https://i.loli.net/2021/09/24/BTdOIXhmJCYlViM.jpg" alt="银河"></p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>支教</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>爱快路由下无法正常解析DNS的问题</title>
    <url>/2022/03/04/ai-kuai-lu-you-xia-wu-fa-zheng-chang-jie-xi-dns-de-wen-ti/</url>
    <content><![CDATA[<blockquote>
<p>最原始的问题是发现300M的电信宽带在经过软路由后下行只有30M,
而上行还保持正常, 在排查中出现了另外的问题: 直连爱快软路由DNS解析失败,
本文解决了这两个问题</p>
</blockquote>
<p>先说结论:</p>
<p>在客户机DNS服务器设置为网关(ikuai)的情况下,
必须设置爱快的DNS加速服务中使用代理模式才能生效,
同时要开启强制客户端DNS代理,
而此时使用爱快的DNS缓存模式无法正常解析DNS</p>
<p><img src="https://s2.loli.net/2022/03/04/nbLDmNeuoryJIFv.png"></p>
<hr>
<p>以下是完整版:</p>
<p>家中稳定运行半年的网络配置如图所示, 出现问题后开始排查问题位置</p>
<p>①查看是否是运营商限制了下行带宽:
测试家中网络硬路由部分的速率发现一切正常, 所以问题出在软路由</p>
<p>②逐个排查软路由内各部分: 依次关闭adg, openwrt没有改善问题,
所以问题可能在ikuai上</p>
<figure>
<img src="https://s2.loli.net/2022/03/04/7X6kQ9LiKyqpxIu.png" alt="家中原始网络拓扑图, 分为经过软路由的改造部分和初始路由器部分">
<figcaption aria-hidden="true">家中原始网络拓扑图,
分为经过软路由的改造部分和初始路由器部分</figcaption>
</figure>
<p>将电脑直连ikuai, 手动设置IP, DNS和网关均为ikuai,
在未做其他修改的情况下出现了无法解析DNS的情况</p>
<figure>
<img src="https://s2.loli.net/2022/03/04/qJOzB1orpgMm54n.png" alt="7X6kQ9LiKyqpxIu">
<figcaption aria-hidden="true">7X6kQ9LiKyqpxIu</figcaption>
</figure>
<blockquote>
<p>qq和微信都是不需要解析DNS的软件,
如果这两个软件能正常使用而网页无法打开就能确定DNS出问题</p>
</blockquote>
<p>先后修改了ikuai和电脑的DNS服务器, 从openwrt内到ikuai再到阿里腾讯,
均不能正常上网</p>
<figure>
<img src="https://s2.loli.net/2022/03/04/brmS398LFOdwzJC.jpg" alt="ikuai内的设置, 无法解析DNS">
<figcaption aria-hidden="true">ikuai内的设置, 无法解析DNS</figcaption>
</figure>
<p>ikuai内的设置, 无法解析DNS</p>
<p>然后怀疑是不是ikuai本身就无法正常上网,
使用ikuai内的测试工具发现能够ping通域名</p>
<p><img src="https://s2.loli.net/2022/03/04/OAGZUVnaegkl3Xi.jpg"></p>
<p>到这里完全没有思路, 之前也没见过路由能解析DNS但主机不能的情况,
在网上和论坛搜索了很久都没有找到相关的情况</p>
<p>最后的最后在ikuai官方的手册里找到了答案</p>
<p><a href="https://www.ikuai8.com/index.php?option=com_content&amp;view=article&amp;id=129:dns&amp;catid=39&amp;Itemid=228">https://www.ikuai8.com/index.php?option=com_content&amp;view=article&amp;id=129:dns&amp;catid=39&amp;Itemid=228</a></p>
<p><img src="https://s2.loli.net/2022/03/04/r9JVAwTgPLbYpqu.jpg"></p>
<p>所以修改了代理模式, 开启了强制代理, 问题解决</p>
<p><img src="https://s2.loli.net/2022/03/04/BHkuZeJ2OarSUNR.jpg"></p>
<blockquote>
<p>回到最初的降速问题,
经历了iperf测速等9981种方法也没有头绪，最后竟然在V2上找到了同样的病友，最后是修改了WiFi的信道和带宽...果然没再出问题，可能是过了个年周围的邻居变多了吧...</p>
</blockquote>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title>生成模型之Flow-based Model(1)前置知识</title>
    <url>/2022/07/08/sheng-cheng-mo-xing-zhi-flow-based-model-1-qian-zhi-zhi-shi/</url>
    <content><![CDATA[<h1 id="flow-based-model前置知识">Flow-based Model前置知识</h1>
<blockquote>
<p>流模型是数学上严密推理得到的模型,本篇为数学上的前置内容</p>
</blockquote>
<h2 id="雅各比矩阵-jacobian-matrix">雅各比矩阵 Jacobian Matrix</h2>
<p>对于两个矩阵<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span><br>
<span class="math display">\[z =
\left[\begin{matrix}z_{1}\\z_{2}\end{matrix}\right]\]</span> <span class="math display">\[x =
\left[\begin{matrix}x_{1}\\x_{2}\end{matrix}\right]\]</span></p>
<p><span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>之间存在关系:<br>
<span class="math inline">\(x = f(z)\)</span>, <span class="math inline">\(z = f^{-1}(x)\)</span></p>
<p>Jacobian Matrix定义为:<br>
<span class="math display">\[
J_{f} = \left[ \begin{matrix}
\partial x_{1}/\partial z_{1} &amp;&amp; \partial x_{1}/\partial z_{2}
\\
\partial x_{2}/\partial z_{1} &amp;&amp; \partial x_{2}/\partial z_{2}
\end{matrix} \right]
\]</span></p>
<p><span class="math display">\[
J_{f^{-1}} = \left[ \begin{matrix}
\partial z_{1}/\partial x_{1} &amp;&amp; \partial z_{1}/\partial x_{2}
\\
\partial z_{2}/\partial x_{1} &amp;&amp; \partial z_{2}/\partial x_{2}
\end{matrix} \right]
\]</span></p>
<p><span class="math display">\[
J_{f}J_{f^{-1}}=1
\]</span></p>
<h2 id="行列式-determinant">行列式 Determinant</h2>
<p>对于一个矩阵<span class="math inline">\(A\)</span><br>
<span class="math display">\[
A = \left[ \begin{matrix}a &amp;&amp; b\\c &amp;&amp;
d\end{matrix}\right]
\]</span> 其行列式结果为:<br>
<span class="math display">\[
\det(A) = ad-bc
\]</span></p>
<p>对于一个矩阵<span class="math inline">\(B\)</span><br>
<span class="math display">\[
B = \left[ \begin{matrix}a_{1} &amp;&amp; a_{2} &amp;&amp; a_{3}\\a_{4}
&amp;&amp; a_{5} &amp;&amp; a_{6}\\a_{7} &amp;&amp; a_{8} &amp;&amp;
a_{9}\end{matrix}\right]
\]</span> 其行列式结果为:<br>
<span class="math display">\[
\det(B) =
a_{1}a_{5}a_{9}+a_{2}a_{6}a_{7}+a_{3}a_{4}a_{8}-a_{3}a_{5}a_{7}-a_{2}a_{4}a_{9}-a_{1}a_{6}a_{8}
\]</span></p>
<p>有公式:<br>
<span class="math display">\[
\det (A) = \frac {1}{\det(A^{-1})}
\]</span></p>
<p><span class="math display">\[
\det (J_{f}) = \frac {1}{\det(J_{f^{-1})}}
\]</span></p>
<h2 id="可变理论-change-of-variable-theorem">可变理论 change of variable
theorem</h2>
<p>对于正态分布<span class="math inline">\(\pi(z)\)</span>和概率分布<span class="math inline">\(p(x)\)</span><br>
其中<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>满足<span class="math inline">\(x=f(z)\)</span></p>
<h3 id="一维形式">一维形式</h3>
<p><span class="math display">\[ p(x&#39;)\Delta x = \pi (z&#39;)\Delta
z\]</span> <span class="math display">\[p(x&#39;) = \pi (z&#39;) |\frac
{\mathrm{d} z}{\mathrm{d} x}| \]</span></p>
<h3 id="二维形式">二维形式</h3>
<p><span class="math display">\[p(x&#39;)|det(J_f)|=\pi(z&#39;)\]</span>
<span class="math display">\[p(x&#39;) =
\pi(z&#39;)|det(J_{f^{-1}})|\]</span></p>
<h3 id="引申">引申</h3>
<p>如果<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>满足<span class="math inline">\(x=f(z)\)</span>,则他们的分布之间关系就是相差<span class="math inline">\(|det(J_f)|\)</span></p>
<h2 id="生成器generator">生成器Generator</h2>
<p>对于一个网络<span class="math inline">\(G\)</span>,我们定义他的分布为<span class="math inline">\(P_{G}\)</span>,对于真实样本的分布为<span class="math inline">\(P_{data}\)</span><br>
对于输入<span class="math inline">\(z\)</span>,经过生成器得到的结果为<span class="math inline">\(x\)</span>,记为<span class="math inline">\(x=G(z)\)</span><br>
说明<span class="math inline">\(x\)</span>服从<span class="math inline">\(P_{G}(x)\)</span>分布
显然一个好的生成器应当使得<span class="math inline">\(P_{G}(x)\)</span>接近<span class="math inline">\(P_{data}(x)\)</span></p>
<p>因此理论上有最优生成器<span class="math inline">\(G^{*}\)</span><br>
<span class="math inline">\(G^{*}= argmax_{G}
\sum\limits_{i=1}^{m}logP_{G}(x^{i})\)</span> 其中 <span class="math inline">\({x^i from P_{data}(x)}\)</span></p>
<p><span class="math inline">\(G^{*} \approx argmin_{G}KL(P_{data} ||
P_{G})\)</span><br>
(KL散度越小,两个分布越接近)</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>生成模型之Flow-based-Model(2)</title>
    <url>/2022/07/09/sheng-cheng-mo-xing-zhi-flow-based-model-2-liu-mo-xing-tui-dao/</url>
    <content><![CDATA[<h1 id="生成模型flow-based-model2流模型">生成模型Flow-Based
Model（2）流模型</h1>
<h2 id="推导">推导</h2>
<p>根据生成器相关知识可知：一个最佳的生成器<span class="math inline">\(G^{*}= argmax_{G}
\sum\limits_{i=1}^{m}logP_{G}(x^{i})\)</span></p>
<p>根据可变理论有：<span class="math inline">\(P_G(x^i)=\pi(z^i)|\det(J_{G^{-1}})|\)</span>,
其中<span class="math inline">\(z^i=G^{-1}(x^i)\)</span></p>
<p>等号两侧取对数有: <span class="math inline">\(logP_G(x^i)=log\pi(G^{-1}(x^i))+log|\det(J_{G^{-1}}|\)</span></p>
<p>由此得到最佳生成器<span class="math inline">\(G^*\)</span>的右式,
即让<span class="math inline">\(logP_G(x^i)\)</span>最大,
便得到最佳<span class="math inline">\(G^*\)</span></p>
<p>然而<span class="math inline">\(z^i=G^{-1}(x^i)\)</span>对<span class="math inline">\(G\)</span>提出了限制:</p>
<ol type="1">
<li><span class="math inline">\(\det(J_G)\)</span>需要可计算</li>
<li><span class="math inline">\(G^{-1}\)</span>要求生成器<span class="math inline">\(G\)</span>本身可逆, 输入输出的形状一样</li>
</ol>
<p>解决方法: 使用多个生成器<span class="math inline">\(G\)</span>连续生成, 以达到拟合<span class="math inline">\(P_{data}\)</span>的目的</p>
<h2 id="多个生成器的情况">多个生成器的情况</h2>
<p>对于多个<span class="math inline">\(G\)</span>的生成结果有: <span class="math inline">\(\log(P_K(x^i))=log\pi(z^i)+\sum\limits_{i=1}^Klog|\det(J_{G_{K^{-1}}})|\)</span>,
其中<span class="math inline">\(z^i=G_1^{-1}G_2^{-1}...G_K^{-1}(x^i)\)</span></p>
<blockquote>
<p>依旧是取左式最大</p>
</blockquote>
<h2 id="流模型实质">流模型实质</h2>
<p>以一个生成器为例:</p>
<pre class="mermaid">graph LR
z --> G --> x</pre>
<p><span class="math display">\[
logP_G(x^i)=log\pi(G^{-1}(x^i))+log|\det(J_{G^{-1}})|
\]</span></p>
<p>发现为得到左式中的<span class="math inline">\(G\)</span>,
右式只是用到了<span class="math inline">\(G^{-1}\)</span>,
因此我们直接训练出<span class="math inline">\(G^{-1}\)</span></p>
<pre class="mermaid">graph RL
x --> G-1 --> z</pre>
<p>其中, <span class="math inline">\(x^i\)</span>是从<span class="math inline">\(P_{data}(x)\)</span>中采样的, <span class="math inline">\(z^i=G^{-1}(x^i)\)</span></p>
<p>欲使左式最大, 右侧两项应尽可能大</p>
<ul>
<li>右侧第一项是对数函数, 而正态分布<span class="math inline">\(\pi(z^i)\)</span>在<span class="math inline">\(z^i\)</span>等于0向量时最大,
因此可能导致训练出的<span class="math inline">\(G^{-1}\)</span>使所有生成的<span class="math inline">\(z^i\)</span>都为零向量, 进而导致<span class="math inline">\(J_{G^{-1}}\)</span>变成全零矩阵, 行列式<span class="math inline">\(\det(J_{G^{-1}})\)</span>也为0,
最终右侧第二项变为<span class="math inline">\(-\inf\)</span></li>
</ul>
<p>所以流模型实际上在做的事情就是: 在第一项趋零的情况下,
使用第二项约束结果防止全零, 最终得到最佳生成器</p>
<h2 id="应用-coupling-layer">应用-Coupling Layer</h2>
<figure>
<img src="https://s2.loli.net/2022/07/10/pqBCRzcYLbiu2Uo.png" alt="coupling-layer">
<figcaption aria-hidden="true">coupling-layer</figcaption>
</figure>
<p>左侧为<span class="math inline">\(z^i\)</span>, 右侧为<span class="math inline">\(x^i\)</span> <span class="math display">\[
\begin{equation}
\left\{
\begin{aligned}
z_{i\leq d}&amp;=x^i\\
z_{i &gt; d}&amp;=\frac {x^i-\gamma^i}{\beta^i}
\end{aligned}
\right.
\end{equation}
\]</span></p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络中的优化算法和正则化问题</title>
    <url>/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/</url>
    <content><![CDATA[<blockquote>
<p>本文主要参考邱锡鹏老师的nndl-book，作为笔记使用</p>
<p><a href="https://www.bilibili.com/video/BV19u411d7r3">神经网络与深度学习（第7-10讲）（更新至：第9讲
无监督学习）_哔哩哔哩_bilibili</a></p>
</blockquote>
<h1 id="非凸优化问题">非凸优化问题</h1>
<p>高维空间中常常遇到的都是非凸函数</p>
<p>非凸优化问题中难点有：</p>
<ul>
<li>如何逃离鞍点</li>
<li>会出现平坦最小值</li>
</ul>
<h2 id="鞍点">鞍点</h2>
<p>在某些维度上是最高点,另一些维度上是最低点.其一阶导数为0,二阶梯度Hessian矩阵不是半正定矩阵.</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled.png" alt="鞍点示例">
<figcaption aria-hidden="true">鞍点示例</figcaption>
</figure>
<h2 id="平坦最小值">平坦最小值</h2>
<blockquote>
<p>由于DL中的参数非常多且具有冗余,单个参数对最终的损失影响比较小,这会导致损失函数在局部最小解附近通常是一个平坦的区域</p>
</blockquote>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%201.png" alt="平坦最小值和尖锐最小值">
<figcaption aria-hidden="true">平坦最小值和尖锐最小值</figcaption>
</figure>
<ul>
<li>一个平坦最小值的邻域内,所有点对应的训练损失都比较接近</li>
<li>大部分局部最小解是等价的</li>
<li>局部最小解对应的训练损失有可能非常接近于全局最小解训练损失</li>
</ul>
<aside>
💡
在训练神经网络时,可以用局部最小解代替全局最优解,以防止过拟合,鲁棒性更好
</aside>
<h1 id="神经网络优化算法">神经网络优化算法</h1>
<h2 id="随机梯度下降法sgd">随机梯度下降法(SGD)</h2>
<p>小批量随机梯度下降,每次选择k个样本计算偏导数的平均数,然后在梯度反向上更新参数</p>
<h2 id="批量大小调整">批量大小调整</h2>
<p>不影响梯度的期望,但会影响方差.</p>
<ul>
<li><p>batch越大,方差越小,学习率需要设置较大</p></li>
<li><p>batch越小,需要设置较小的学习率,否则模型不会收敛</p></li>
<li><p>NOTE:batch和学习率可遵守线性缩放原则</p>
<p>batch变为m倍,学习率变为<span class="math inline">\(1/m\)</span></p></li>
</ul>
<aside>
💡
batch越大通常训练的效率越高,batch越小由于随机性更高,可能会获得更好的泛化能力
</aside>
<h2 id="梯度估计修正">梯度估计修正</h2>
<ul>
<li><p>Momentum Method 动量法</p>
<p>用之前累积的动量来代替真正的梯度（负梯度的加权移动平均）</p>
<aside>
<p>💡 每次的迭代方向相当于加速度，类似二阶梯度</p>
</aside>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta_t =\rho \Delta \theta_{t-1}-\alpha g_t=-\alpha \sum
\limits _{\tau=1}^{t} \rho^{t-\tau}g_{\tau}
  \]</span></p></li>
<li><p>Nesterov 加速梯度</p></li>
<li><p>梯度截断</p>
<p>当某个时刻梯度（的模）过大如NaN，就把梯度的模限定在某个区间，将超过区间的部分进行截断</p>
<ul>
<li><p>按值截断</p>
<p>例如取<span class="math inline">\([a,b]\)</span>区间的值</p>
<p><span class="math display">\[
  g_t=\max(\min(g_t,b),a)
  \]</span></p></li>
<li><p>按模截断</p>
<p>把模的最大值限定为b</p>
<p><span class="math display">\[
  g_t=\frac{b}{||g_t||}g_t
  \]</span></p></li>
</ul></li>
<li><p>Adam算法</p>
<p>使用动量法优化梯度，加上RMSprop自适应调整学习率</p>
<p>先计算两个移动平均：</p>
<p><span class="math display">\[
  M_t = \beta _1M_{t-1}+(1-\beta_1)g_t
  \]</span></p>
<p><span class="math display">\[
  G_t = \beta_2G_{t-1}+(1-\beta_2)g_t \odot g_t
  \]</span></p>
<p>当t=1时，<span class="math inline">\(M_1\)</span>应该等于<span class="math inline">\(g_1\)</span>，但是在上述公式中不正确，于是加入偏差修正如下：</p>
<p><span class="math display">\[
  \hat M_t = \frac {M_t}{1-\beta_1^t}
  \]</span></p>
<p><span class="math display">\[
  \hat G_t=\frac{G_t}{1-\beta_2^t}
  \]</span></p>
<p>更新参数内容：</p>
<p><span class="math display">\[
  \Delta \theta_t = - \frac{\alpha}{\sqrt {G_t+\epsilon}}\hat M_t
  \]</span></p></li>
</ul>
<h2 id="学习率调整">学习率调整</h2>
<ul>
<li><p>学习率衰减</p>
<ul>
<li>阶梯衰减</li>
<li>逆时衰减:<span class="math inline">\(\alpha_t = \alpha_0
\frac{1}{1+\beta \times t}\)</span></li>
<li>指数衰减</li>
<li>余弦衰减</li>
</ul></li>
<li><p>周期性学习率调整</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%202.png" alt="周期性学习率调整">
<figcaption aria-hidden="true">周期性学习率调整</figcaption>
</figure>
<p>跳出局部最优或鞍点的情况,找到更平坦的局部最优</p></li>
<li><p>学习率预热</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%203.png" alt="学习率预热示例">
<figcaption aria-hidden="true">学习率预热示例</figcaption>
</figure></li>
<li><p>AdaGrad算法</p>
<p>借鉴L2正则化的思想，在第t次迭代的时候，先计算每个参数梯度平方的累计值。</p>
<p><span class="math display">\[
  G_t=\sum \limits _{\tau}^{t}g_{\tau}\odot g_{\tau}
  \]</span></p>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta _{t} = - \frac {\alpha}{\sqrt {G_t+\epsilon}}\odot g_t
  \]</span></p>
<p>其中<span class="math inline">\(\alpha\)</span>是初始学习率，<span class="math inline">\(\epsilon\)</span>是为保持数值稳定一个小常数（大小通常是<span class="math inline">\(e^{-7}\sim
e^{-10}\)</span>），公式内的运算都是按元素进行的操作。</p>
<blockquote>
<p>from nndl-book:</p>
<p>在 AdaGrad
算法中，如果某个参数的偏导数累积比较大，其学习率相对较小;相反，如果其偏导数累积较小，其学习率相对较大.但整体是随着迭代次数的增加，学习率逐渐缩小.
AdaGrad
算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点.</p>
</blockquote></li>
<li><p>RMSprop算法</p>
<p>避免AdaGrad算法中学习率不断单调下降导致过早衰减的问题。</p>
<p>参数更新的差值和AdaGrad算法是相同的：</p>
<p><span class="math display">\[
  \Delta \theta _{t} = - \frac {\alpha}{\sqrt {G_t+\epsilon}}\odot g_t
  \]</span></p>
<p>区别在于计算<span class="math inline">\(G_t\)</span>:</p>
<p><span class="math display">\[
  \begin{aligned}
  G_{t} &amp;=\beta G_{t-1}+(1-\beta) \boldsymbol{g}_{t} \odot
\boldsymbol{g}_{t} \\
  &amp;=(1-\beta) \sum_{\tau=1}^{t} \beta^{t-\tau} \boldsymbol{g}_{\tau}
\odot \boldsymbol{g}_{\tau}
  \end{aligned}
  \]</span></p>
<p>由于参数的学习率变化由单调衰减变成了指数加权的移动平均，既可以变大也可以变小。</p></li>
<li><p>AdaDelta算法</p>
<p>参数更新的差值为：</p>
<p><span class="math display">\[
  \Delta \theta_{t}=-\frac{\sqrt{\Delta
X_{t-1}^{2}+\epsilon}}{\sqrt{G_{t}+\epsilon}} \mathbf{g}_{t}
  \]</span></p>
<p>其中<span class="math inline">\(G_t\)</span>和RMSprop算法相同：</p>
<p><span class="math display">\[
  \begin{aligned}
  G_{t} &amp;=\beta G_{t-1}+(1-\beta) \boldsymbol{g}_{t} \odot
\boldsymbol{g}_{t} \\
  &amp;=(1-\beta) \sum_{\tau=1}^{t} \beta^{t-\tau} \boldsymbol{g}_{\tau}
\odot \boldsymbol{g}_{\tau}
  \end{aligned}
  \]</span></p>
<p>分子上：</p>
<p><span class="math display">\[
  \Delta X_{t-1}^{2}=\beta_{1} \Delta
X_{t-2}^{2}+\left(1-\beta_{1}\right) \Delta \theta_{t-1} \odot \Delta
\theta_{t-1}
  \]</span></p>
<p>其中<span class="math inline">\(\beta _1\)</span>是衰减率</p>
<p>由于将原本的学习率修改为了带衰减率的移动平均，能够抑制住学习率的波动。</p></li>
</ul>
<h2 id="参数初始化">参数初始化</h2>
<p>当使用梯度下降法优化网络参数时，参数的初始值非常重要，直接影响到网络的优化效率和泛化能力</p>
<p>神经网络的参数不能初始化为0，会出现对称权重问题，导致层内所有权重相同。</p>
<p>因此对于参数的初始化也有许多方法。</p>
<ul>
<li>预训练初始化</li>
<li>随机初始化
<ul>
<li><p>基于固定方差的参数初始化</p>
<ul>
<li><p>高斯分布初始化</p>
<p>使用方差和均值的高斯分布</p></li>
<li><p>均匀分布初始化</p>
<p>采用在区间<span class="math inline">\([-r,r]\)</span>内采用均匀分布进行初始化</p></li>
</ul></li>
<li><p>基于方差缩放的参数初始化</p>
<p><span class="math inline">\(M_l\)</span>是第<span class="math inline">\(l\)</span>层的神经元个数</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%204.png" alt="基于方差缩放的参数初始化设置">
<figcaption aria-hidden="true">基于方差缩放的参数初始化设置</figcaption>
</figure></li>
<li><p>正交初始化</p>
<p>由于上述两种初始化对每个参数都是独立采样，依旧存在梯度消失或梯度爆炸问题。</p>
<p>此处我们从最开始就构建一个L层的等宽线性网络：</p>
<p><span class="math display">\[
  \bold y= \bold W^L \bold W^{L-1} \dots \bold W^1 \bold x
  \]</span></p>
<p>在反向传播中误差项<span class="math inline">\(\delta\)</span>的反向传播公式为：<span class="math inline">\(\delta ^{l-1}=(W^l)^T\delta ^l\)</span></p>
<p><strong>范数保持性：</strong>指误差项在反向传播中的范数保持不变，满足<span class="math inline">\(||\delta^{l-1}||^2=||\delta^l||^2=||(W^l)^T\delta^l||^2\)</span>。</p>
<p>于是在初始化就可以使用均值为0，方差为<span class="math inline">\(1/M\)</span>的高斯分布来做初始化</p>
<hr>
<p>或者采用更直接的方式：直接将 <span class="math inline">\(W^l\)</span>初始化为正交矩阵，即<span class="math inline">\(W^l(W^l)^T=I\)</span></p>
<ol type="1">
<li>y地方用均值为0，方差为1的高斯分布初始化一个矩阵</li>
<li>将这个矩阵用奇异值分解得到两个正交矩阵，用其中之一作为权重矩阵</li>
</ol>
<aside>
<p>💡 正交初始化通常用在RNN的循环边上的权重矩阵</p>
</aside></li>
</ul></li>
</ul>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="尺度不变性scale-invariance">尺度不变性（Scale Invariance）</h3>
<p>对于机器算法中的数据，在缩放部分特征或者全部特征后，不影响学习和预测。</p>
<p>例如：将厘米单位的特征，缩放为毫米单位。</p>
<p><em>NOTE:KNN算法的尺度不变性较差，因为其计算的欧式距离对数据要求高</em></p>
<p>不同输入特征的尺度差异较大会带来两个问题：</p>
<ol type="1">
<li>参数初始化困难</li>
<li>优化效率较低</li>
</ol>
<h3 id="归一化规范化-normalization">归一化（规范化 Normalization）</h3>
<ul>
<li><p><strong>最小最大值规范化</strong>：</p>
<p>找到数据中的最小最大值，将所有数据缩放到这个范围内</p></li>
<li><p><strong>标准化（Z值规范化）</strong>：</p>
<p>对每一维数据计算均值和方差</p>
<p><span class="math display">\[
  \mu = \frac{1}{N} \sum \limits _{n=1}^Nx^n
  \]</span></p>
<p><span class="math display">\[
  \sigma ^2 = \frac{1}{N}\sum \limits _{n=1}^N (x^n-\mu)^2
  \]</span></p>
<p>然后将特征<span class="math inline">\(x^n\)</span>进行变换：</p>
<p><span class="math display">\[
  \hat x^n = \frac{x^n-\mu}{\sigma}
  \]</span></p>
<p>标准差<span class="math inline">\(\sigma\)</span>做分母不为零的含义：</p>
<p>表示这一维度的特征相同，不重要可以直接去掉</p></li>
<li><p><strong>白化（whitening，PCA）</strong>:</p>
<p>去除所有成分之间的相关性</p>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%205.png" alt="标准化和白化">
<figcaption aria-hidden="true">标准化和白化</figcaption>
</figure></li>
</ul>
<h3 id="逐层规范化">逐层规范化</h3>
<p>ML中的常用方法，应用到DL的目的是：</p>
<ol type="1">
<li><p>更好的尺度不变性</p>
<p>解决内部协变量偏移问题，防止在深度学习中某一个层的输入分布的微小变化引起更深层中的较大偏移。</p></li>
<li><p>更平滑的优化地形</p></li>
</ol>
<ul>
<li>规范化方法
<ul>
<li><p><strong>批量规范化（Batch Normalization）</strong></p>
<p>对于一个神经网络，第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(z^l\)</span>，神经元的输出为<span class="math inline">\(a^l\)</span>，即：</p>
<p><span class="math display">\[
  a^l = f(z^l)=f(Wa^{l-1}+b)
  \]</span></p>
<p>对于一个样本数为k的批量样本，计算其方差和均值：</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;\boldsymbol{\mu}_{\mathcal{B}}=\frac{1}{K} \sum_{k=1}^{K}
\boldsymbol{z}^{(k, l)} \\
  &amp;\boldsymbol\sigma_{\mathcal{B}}^{2}=\frac{1}{K}
\sum_{k=1}^{K}\left(\boldsymbol{z}^{(k,
l)}-\boldsymbol{\mu}_{\mathcal{B}}\right) \odot\left(\boldsymbol{z}^{(k,
l)}-\boldsymbol{\mu}_{\mathcal{B}}\right) .
  \end{aligned}
  \]</span></p>
<p>批量规范化：</p>
<p><span class="math display">\[
  \begin{aligned}
  \hat{\boldsymbol{z}}^{l}
&amp;=\frac{\boldsymbol{z}^{l}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}
\odot \boldsymbol{\gamma}+\boldsymbol{\beta} \\
  &amp; \triangleq \mathrm{BN}_{\gamma,
\boldsymbol{\beta}}\left(\boldsymbol{z}^{l}\right),
  \end{aligned}
  \]</span></p>
<p>其中<span class="math inline">\(\boldsymbol{\gamma}\)</span>是缩放参数向量，<span class="math inline">\(\boldsymbol{\beta}\)</span>是平移参数向量，都是可学习参数。能够增强规范化的能力。</p>
<aside>
<p>💡 批量规范化可以作为神经网络的一层（BN层）直接加入网络结构中。</p>
</aside></li>
<li><p><strong>层规范化（Layer Normalization）</strong></p>
<p>不看batch的内容，对一层内所有神经元进行规范化</p>
<p>计算层内均值和方差：</p>
<p><span class="math display">\[
  \begin{aligned}
  \mu^{l} &amp;=\frac{1}{M_{l}} \sum_{i=1}^{M_{l}} z_{i}^{l} \\
  (\sigma^{l})^{2} &amp;=\frac{1}{M_{l}}
\sum_{i=1}^{M_{l}}\left(z_{i}^{l}-\mu^{l}\right)^{2}
  \end{aligned}
  \]</span></p>
<p>层规范化：</p>
<p><span class="math display">\[
  \begin{aligned}
  \hat{\boldsymbol{z}}^{l}
&amp;=\frac{\boldsymbol{z}^{l}-\mu^{l}}{\sqrt{(\sigma^{(l)})^{2}+\epsilon}}
\odot \boldsymbol{\gamma}+\boldsymbol\beta \\
  &amp; \triangleq \mathrm{LN}_{\boldsymbol {\gamma,
\beta}}\left(\boldsymbol{z}^{l}\right)
  \end{aligned}
  \]</span></p>
<aside>
<p>💡
层规范化也可以作为神经网络的一层（LN层）直接加入网络结构中。应用较多。</p>
</aside>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%206.png" alt="BN和LN的区别">
<figcaption aria-hidden="true">BN和LN的区别</figcaption>
</figure>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%207.png" alt="各种规范化的对比">
<figcaption aria-hidden="true">各种规范化的对比</figcaption>
</figure></li>
<li><p>权重规范化</p></li>
<li><p>局部响应规范化</p></li>
</ul></li>
</ul>
<h2 id="超参数优化">超参数优化</h2>
<h3 id="超参数">超参数</h3>
<ul>
<li>层数</li>
<li>每层神经元个数</li>
<li>激活函数</li>
<li>学习率（以及动态调整算法）</li>
<li>正则化系数</li>
<li>mini-batch大小</li>
</ul>
<h3 id="优化方法">优化方法</h3>
<ul>
<li><p>网格搜索</p>
<p>对每一个超参数取几个”经验值“，比如对于学习率<span class="math inline">\(\alpha\)</span>，可以设置</p>
<p><span class="math display">\[
  \alpha  \in \{0.01,0.1,0.5,1.0\}
  \]</span></p></li>
<li><p>随机搜索</p>
<p>对超参数进行随机组合</p></li>
<li><p>贝叶斯优化</p></li>
<li><p>动态资源分配</p></li>
<li><p>神经架构搜索</p></li>
</ul>
<h2 id="网络正则化">网络正则化</h2>
<p>由于神经网络的拟合能力强，会将模型过度参数化，会导致模型泛化性差。为提高神经网络的泛化能力通常要加入正则化。</p>
<p><strong>正则化</strong>就是所有损害优化方法的方法。通常分为两种方式：</p>
<ol type="1">
<li>增加优化约束：L1、L2约束</li>
<li>干扰优化过程：提前停止、随机梯度下降、暂退法、权重衰减。</li>
</ol>
<h3 id="早停法">早停法</h3>
<p>引入<strong>验证集（Validation
Dataset）</strong>测试每一次迭代的参数时候在验证集上最优，如果在验证集上错误率不再下降就停止迭代。防止过拟合。</p>
<h3 id="权重衰减">权重衰减</h3>
<p>限制权重取值范围。在每次参数更新时，引入一个衰减系数<span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[
\theta_t=(1-\beta)\theta_{t-1}-\alpha \bold g_t
\]</span></p>
<h3 id="暂退法dropout">暂退法（Dropout）</h3>
<p>有一个神经层<span class="math inline">\(y=f(Wx+b)\)</span>，引入一个随机的掩蔽函数<span class="math inline">\(mask(x)=m\odot x\)</span>，<span class="math inline">\(m \in \{0,1\}^D\)</span>.得到神经层的输出<span class="math inline">\(y=f(Wmask(x)+b)\)</span>.</p>
<aside>
💡
可以理解为从原始网络中采样得到一个子网络，如果有n个神经元，则可以采样出<span class="math inline">\(2^n\)</span>个子网络。
</aside>
<h3 id="ell_1和ell_2正则化"><span class="math inline">\(\ell_1\)</span>和<span class="math inline">\(\ell_2\)</span>正则化</h3>
<p>在原本的优化问题中加入<span class="math inline">\(\ell_1\)</span><span class="math inline">\(\ell_2\)</span>范数防止过拟合</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^N
\mathcal{L}\left(y^{(n)}, f\left(\boldsymbol{x}^{(n)} ;
\theta\right)\right) \\
&amp;\text { s.t. } \quad \ell_{p}(\theta) \leq 1
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\ell_2\)</span>正则化的参数更新结果为：<span class="math inline">\(\theta_t =
\theta_{t-1}-\alpha(g_t+\lambda\theta_{t-1})=(1-\alpha\lambda)\theta_{t-1}-\alpha
g_t\)</span></p>
<p>形式上和权重衰减的类似：<span class="math inline">\(\theta_t=(1-\beta)\theta_{t-1}-\alpha \bold
g_t\)</span></p>
<aside>
💡 在SGD中 <span class="math inline">\(\ell_2\)</span>
正则化和权重衰减是等价的，但在复杂的优化算法如Adam中则不能等价。
</aside>
<figure>
<img src="/2022/07/31/shen-jing-wang-luo-zhong-de-you-hua-suan-fa-he-zheng-ze-hua-wen-ti/Untitled%208.png" alt="不同范数的约束条件示例">
<figcaption aria-hidden="true">不同范数的约束条件示例</figcaption>
</figure>
<h1 id="总结">总结</h1>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th>优化</th>
<th>正则化</th>
<th>隐式正则化</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>用ReLU作为激活函数</td>
<td>SGD+mini-batch(动态学习率、Adam算法优先)</td>
<td>早停法</td>
<td>SGD</td>
</tr>
<tr class="even">
<td>残差链接</td>
<td>每次迭代都重新随机排序</td>
<td>暂退法</td>
<td>批量规范化</td>
</tr>
<tr class="odd">
<td>逐层规范化</td>
<td>数据预处理（规范化）</td>
<td>权重衰减</td>
<td><span class="math inline">\(\ell_1\)</span>和<span class="math inline">\(\ell_2\)</span>正则化</td>
</tr>
<tr class="even">
<td></td>
<td>参数初始化（预训练）</td>
<td></td>
<td>数据增强</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Apple Silicon MacBook Pro新机配置</title>
    <url>/2022/07/31/apple-silicon-macbook-pro-xin-ji-pei-zhi/</url>
    <content><![CDATA[<blockquote>
<p>本文记录下由intel转移到m1的过程中重装各种环境和工具的命令和配置</p>
</blockquote>
<aside>
💡 第一次使用苹果的time
machine进行迁移，给我的感觉就像把旧电脑的硬盘直接摆在面前一样，迁移文件很好用
</aside>
<h1 id="配置git">配置git</h1>
<p>mac自带git，所以只需要配置下用户名和邮箱</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;John Doe&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;johndoe@example.com&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="安装brew">安装brew</h1>
<p><a href="https://brew.sh/index_zh-cn">Homebrew</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin/bash -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<p>根据返回的最后内容执行提示的代码，将brew添加到path中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;&#x27;</span> &gt;&gt; /Users/ashone/.zprofile</span><br></pre></td></tr></table></figure>
<p>最后在终端中输入brew测试是否添加成功</p>
<h1 id="安装iterm2">安装iTerm2</h1>
<p><a href="https://iterm2.com/index.html">iTerm2 - macOS Terminal
Replacement</a></p>
<h1 id="安装oh-my-zsh">安装oh-my-zsh</h1>
<p><a href="https://ohmyz.sh/#install">Oh My Zsh - a delightful &amp;
open source framework for Zsh</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="安装zsh主题">安装zsh主题</h1>
<p>我选择安装powerlevel10k主题，链接在这里https://github.com/romkatv/powerlevel10k</p>
<ol type="1">
<li><p>安装字体（可跳过）</p>
<p><a href="https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS%20NF%20Regular.ttf">MesloLGS
NF Regular.ttf</a></p>
<p><a href="https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS%20NF%20Bold.ttf">MesloLGS
NF Bold.ttf</a></p>
<p><a href="https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS%20NF%20Italic.ttf">MesloLGS
NF Italic.ttf</a></p>
<p><a href="https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS%20NF%20Bold%20Italic.ttf">MesloLGS
NF Bold Italic.ttf</a></p></li>
<li><p>手动安装主题文件</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/romkatv/powerlevel10k.git <span class="variable">$&#123;ZSH_CUSTOM:-<span class="variable">$HOME</span>/.oh-my-zsh/custom&#125;</span>/themes/powerlevel10k</span><br></pre></td></tr></table></figure></p></li>
<li><p>编辑配置文件，修改主题为10k</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">code ~/.zshrc</span><br></pre></td></tr></table></figure></p>
<p>也可以使用其他编辑器打开这个文件</p>
<p>然后找到第11行修改主题为</p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">powerlevel10k/powerlevel10k</span><br></pre></td></tr></table></figure></p></li>
<li><p>重启iterm，加载主题文件</p>
<p><img src="/2022/07/31/apple-silicon-macbook-pro-xin-ji-pei-zhi/Untitled.png"></p></li>
<li><p>依次根据样式进行配置</p></li>
<li><p>重启iterm就可以看到崭新的主题了</p></li>
</ol>
<h1 id="安装oh-my-zsh自动补全插件">安装oh-my-zsh自动补全插件</h1>
<p><a href="https://github.com/zsh-users/zsh-autosuggestions">GitHub -
zsh-users/zsh-autosuggestions: Fish-like autosuggestions for zsh</a></p>
<ol type="1">
<li><p>下载插件</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure></p></li>
<li><p>编辑zshrc配置文件，增加插件，修改配色</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">code ~/.zshrc</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugins=( </span><br><span class="line">    # other plugins...</span><br><span class="line">    zsh-autosuggestions</span><br><span class="line">)</span><br><span class="line"># 此处修改autosuggestions插件的配色</span><br><span class="line">ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&quot;fg=#d0d0d0,underline&quot;</span><br></pre></td></tr></table></figure></p></li>
<li><p>重开iterm</p></li>
</ol>
<p>查看效果：</p>
<p><img src="/2022/07/31/apple-silicon-macbook-pro-xin-ji-pei-zhi/Untitled%201.png"></p>
<h1 id="配置python和conda">配置python和conda</h1>
<p><a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda -
Conda documentation</a></p>
<h2 id="安装miniconda对应版本的pkg双击后直接安装在终端测试conda命令是否正常">安装miniconda对应版本的pkg，双击后直接安装，在终端测试conda命令是否正常</h2>
<h2 id="查看conda环境-创建新python环境">查看conda环境,
创建新python环境</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda <span class="built_in">env</span> list</span><br><span class="line">conda create -n your_env_name python=x.x</span><br><span class="line"></span><br><span class="line">conda activate dl</span><br><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h1 id="安装pytorchapple-silicon版本">安装pytorch（Apple
Silicon版本）</h1>
<p><a href="https://pytorch.org/">PyTorch</a></p>
<p><img src="/2022/07/31/apple-silicon-macbook-pro-xin-ji-pei-zhi/Untitled%202.png"></p>
<p>pytorch从1.13版本开始支持苹果芯片gpu加速,目前（7.30）还得安装nightly版本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pytorch -c pytorch-nightly</span><br></pre></td></tr></table></figure>
<p>用python测试查看版本号，torch版本为1.13开发版就代表安装成功</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ python</span><br><span class="line">Python 3.8.13 (default, Mar 28 2022, 06:13:39)</span><br><span class="line">[Clang 12.0.0 ] :: Anaconda, Inc. on darwin</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.__version__</span><br><span class="line"><span class="string">&#x27;1.13.0.dev20220729&#x27;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Apple</tag>
        <tag>配置</tag>
        <tag>conda</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo安装和配置插件</title>
    <url>/2022/08/01/hexo-an-zhuang-he-pei-zhi-cha-jian/</url>
    <content><![CDATA[<h1 id="Hexo安装和配置插件"><a href="#Hexo安装和配置插件" class="headerlink" title="Hexo安装和配置插件"></a>Hexo安装和配置插件</h1><aside>
💡 本文安装配置了hexo博客的常用插件，包括pandoc、mermaid、本地图片等插件
</aside>

<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="安装node环境"><a href="#安装node环境" class="headerlink" title="安装node环境"></a>安装node环境</h2><p>可以直接在nodejs官网下载安装包</p>
<h2 id="准备路径后安装hexo"><a href="#准备路径后安装hexo" class="headerlink" title="准备路径后安装hexo"></a>准备路径后安装hexo</h2><p>在想要安装node的路径下建立一个新的文件夹，避免装在根目录下</p>
<p>在新的文件夹下执行命令安装hexo</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo</span><br></pre></td></tr></table></figure>

<p>测试hexo命令</p>
<h1 id="安装各种插件"><a href="#安装各种插件" class="headerlink" title="安装各种插件"></a>安装各种插件</h1><h2 id="pandoc渲染latex公式"><a href="#pandoc渲染latex公式" class="headerlink" title="pandoc渲染latex公式"></a>pandoc渲染latex公式</h2><p>先在官网安装pandoc的包</p>
<p><a href="https://pandoc.org/installing.html">Installing pandoc</a></p>
<p>再执行命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>

<h2 id="安装mermaid流程图插件"><a href="#安装mermaid流程图插件" class="headerlink" title="安装mermaid流程图插件"></a>安装mermaid流程图插件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure>

<p>安装后不能直接使用，需要修改部分文件，修改内容如下</p>
<p>参考文章：</p>
<p><a href="https://ash-one.github.io/2022/07/12/hexo-bo-ke-zai-matery-zhu-ti-xia-cha-ru-mermaid-liu-cheng-tu/">Hexo博客在matery主题下插入mermaid流程图</a></p>
<h2 id="安装支持本地图片的插件"><a href="#安装支持本地图片的插件" class="headerlink" title="安装支持本地图片的插件"></a>安装支持本地图片的插件</h2><ol>
<li><p>插件安装</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>由于插件内容老旧失效，找到文件进行修改</p>
<p> 位置在&#x2F;node_modules&#x2F;hexo-asset-image&#x2F;index.js</p>
 <figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> cheerio = <span class="built_in">require</span>(<span class="string">&#x27;cheerio&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getPosition</span>(<span class="params">str, m, i</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> str.<span class="title function_">split</span>(m, i).<span class="title function_">join</span>(m).<span class="property">length</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> version = <span class="title class_">String</span>(hexo.<span class="property">version</span>).<span class="title function_">split</span>(<span class="string">&#x27;.&#x27;</span>);</span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">filter</span>.<span class="title function_">register</span>(<span class="string">&#x27;after_post_render&#x27;</span>, <span class="keyword">function</span>(<span class="params">data</span>)&#123;</span><br><span class="line">  <span class="keyword">var</span> config = hexo.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">if</span>(config.<span class="property">post_asset_folder</span>)&#123;</span><br><span class="line">    	<span class="keyword">var</span> link = data.<span class="property">permalink</span>;</span><br><span class="line">	<span class="keyword">if</span>(version.<span class="property">length</span> &gt; <span class="number">0</span> &amp;&amp; <span class="title class_">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">	   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line">	<span class="comment">// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span></span><br><span class="line">	<span class="keyword">var</span> endPos = link.<span class="title function_">lastIndexOf</span>(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>;</span><br><span class="line">    link = link.<span class="title function_">substring</span>(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> toprocess = [<span class="string">&#x27;excerpt&#x27;</span>, <span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;content&#x27;</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; toprocess.<span class="property">length</span>; i++)&#123;</span><br><span class="line">      <span class="keyword">var</span> key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">var</span> $ = cheerio.<span class="title function_">load</span>(data[key], &#123;</span><br><span class="line">        <span class="attr">ignoreWhitespace</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">xmlMode</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">lowerCaseTags</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">decodeEntities</span>: <span class="literal">false</span></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(<span class="string">&#x27;img&#x27;</span>).<span class="title function_">each</span>(<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line">		<span class="keyword">if</span> ($(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>))&#123;</span><br><span class="line">			<span class="comment">// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span></span><br><span class="line">			<span class="keyword">var</span> src = $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>).<span class="title function_">replace</span>(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">			<span class="keyword">if</span>(!<span class="regexp">/http[s]*.*|\/\/.*/</span>.<span class="title function_">test</span>(src) &amp;&amp;</span><br><span class="line">			   !<span class="regexp">/^\s*\//</span>.<span class="title function_">test</span>(src)) &#123;</span><br><span class="line">			  <span class="comment">// For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span></span><br><span class="line">			  <span class="comment">// In addition, to support multi-level local directory.</span></span><br><span class="line">			  <span class="keyword">var</span> linkArray = link.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line">				<span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  <span class="keyword">var</span> srcArray = src.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line">				<span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span> &amp;&amp; elem != <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  <span class="keyword">if</span>(srcArray.<span class="property">length</span> &gt; <span class="number">1</span>)</span><br><span class="line">				srcArray.<span class="title function_">shift</span>();</span><br><span class="line">			  src = srcArray.<span class="title function_">join</span>(<span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">			  $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>, config.<span class="property">root</span> + link + src);</span><br><span class="line">			  <span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;update link as:--&gt;&quot;</span>+config.<span class="property">root</span> + link + src);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;no src attr, skipped...&quot;</span>);</span><br><span class="line">			<span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>($(<span class="variable language_">this</span>));</span><br><span class="line">		&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.<span class="title function_">html</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开hexo配置文件_config.yml，找到这个配置项开启功能，目的是在每次使用hexo new生成新博客时，自动生成一个同名的目录存放静态文件</p>
 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>可以生成测试是否能正常加载blog中的本地图片</p>
<p>如果没能正确生成可以使用命令清理文件重新编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>

<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://juejin.cn/post/7006594302604214280">解决hexo引用本地图片无法显示的问题 - 掘金</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>配置</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
